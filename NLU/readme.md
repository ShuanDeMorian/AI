# NLU(Natural Language Understanding)
'자연어 이해' 자연어 표현을 기계가 이해할 수 있는 다른 표현으로 변환시키는 것.
형태소 분석이나 구문 분석과 같은 자연어 처리(NLP, Natural Language Processing)와 혼용해서 사용되는 경우가 많지만, 사실상 NLU가 더 큰 개념이다. 
단순히 단어나 문장의 형태만 기계가 인식하도록 하는 것이 아니라 '의미를 인식하도록 하는 것'을 의미.
NLU 기능의 예로는 문장의 의도 분류, 서로 다른 언어 간 번역 문장 생성, 자연어 질문에 대한 답변 추출 등이 있음.

## 전통적 방식(룰 기반 또는 머신러닝 기반)의 한계
기존 NLU는 통계,룰 기반으로 한계가 존재함, 사람이 직접 추출한(hand-crafted) 특징(Feature)에 강하게 의존함, 추출하는 데 시간이 많이 소요되고, 예외 처리 등에 취약함. 또한 처리 대상 문장이나 문서가 길어질수록 정확도가 하락하는 경향이 있고, 처리 가능한 문서도 정형화된 텍스트에 한정된다는 특징이 있음. 규칙 기반의 수작업 특성상 처음 등장하는 신조어, 오타 등에 취약한 단점이 있음. 
머신러닝 기반의 NLU 기법은 기계가 자동으로 모델을 학습하면서 룰 기반 방식의 공수를 많이 덜 수 있었지만 모델의 구조(모델이 단순한 1차 함수인지, 더 고차원인지, 혹은 로지스틱 곡선을 그리는지 등)를 미리 지정해줘야 하는 등 작업자의 개입이 필요함

## 딥러닝 방식의 개선
딥러닝 기반의 NLU 방식은 데이터로부터 Feature를 자동으로 학습하는 방식임. 기존보다 폭넓은 문맥 정보 처리가 가능하고, 사진, 음성 등과 같이 다른 분야의 모델들과 연결한 Multi-modal 모델 구축이 쉬워진 장점이 있음. 예를 들어 이미지를 Input으로 받아서 간략히 설명하는 캡션을 생성하는 모델이 있음.
기존에 학습하지 않은 신조어나 오타에 robust한 처리 기술이 있어서 룰 기반 방식의 한계를 보완할 수 있음. 또한, 모델의 구조를 미리 지정하지 않고 학습을 통해 모델을 만들어서 작업자의 개입이 최소화되며, 복잡하고 깊은 구조를 만들 수 있어 기존 방식보다 정확도가 높음

최근 수년간 밀집된 벡터 표상에 기반한 인공신경망(Artificial neural network)이 다양한 자연어 처리 task에서 우수한 성능을 보여줌. 이러한 트렌드는 워드 임베딩과 딥러닝 기술의 성공에 힘입은 것. 딥러닝은 특징들을 자동으로 추출하고 표현할 수 있게 함. 이러한 장점으로 인해 딥러닝 기반의 NLU 기법 연구가 활발해짐

## 사람의 말과 글을 이해하도록 하는 방법- 워드 임베딩(Word embedding)
NLU를 위해서 가장 먼저 해야 할 일은 사람의 말과 같은 비정형 텍스트 데이터를 컴퓨터가 인식하고 계산할 수 있도록 정량화하는 것.
워드 임베딩(Word embedding)은 단어나 형태소를 벡터화하는 기술이다. 단어를 벡터로 만들었다면 문장이나 문서를 매트릭스로 변환하는 작업이 가능해짐. 이렇게 되면 비로소 문장 감성 분류나, 기계 번역 드오가 같은 task를 딥러닝 모델로 학습할 수 있게 됨.
자연어로 표현된 단어를 벡터화하는 과정이라 word2vector로 표현하기도 함. 단어를 벡터화하는 가장 단순하며 원시적인 방식으로는 One-hot 벡터 인코딩이 기본적.
One-hot 벡터란 k개의 단어가 있을 때, k-dimension의 0 벡터를 만들고 해당 단어의 인덱스만 1로 표현하는 방식. 주어진 문장을 one-hot 벡터로 표현하려면 우선 중복을 제거한 총 단어들을 나열하여 각각을 one-hot 벡터로 만들고, 이렇게 만든 one-hot 벡터로 주어진 문장을 새로이 표현하여 재구성한다. 
one-hot 벡터 인코딩 방식의 단점은, 모든 단어를 one-hot 벡터로 만든다면, 벡터가 담은 정보에 비교해 처리해야 하는 차원이 몹시 커져 연산 비효율이 발생한다는 점. 표준국어대사전 기준으로 한국어는 약 51만 단어가 있으며, 숫자나 기호, 자주 쓰이는 영어 단어 등을 모두 다루려면 기계가 처리해야 할 벡터의 차원이 굉장히 커짐. 하지만 각 벡터의 대부분은 0 값으로, 담고 있는 정보(특정 인덱스에 위치한 1 값)에 비해 보유한 차원이 비효율적으로 비대함
이러한 저장공간의 단점을 보완하기 위해, 인공신경망을 이용하여 고차원의 one-hot 벡터를 조금 더 응집된 형태의 저차원 벡터로 변환해주는 워드 임베딩 기술이 발전함. 이는 여러 문장을 모델에 제공하고 문장의 문맥을 통해 단어의 의미를 학습시키는 방식임. 이 기술엔 CBOW, SKIP-GRAM 등이 존재함

### 워드 임베딩을 이용해서 얻을 수 있는 장점
#### 1. 유사 의미를 가진 벡터끼리 유사한 공간에 모이는 특징이 발생
'apple'의 경우 동사인 'have'보단 apple과 같이 과일을 나타내는 명사 'pineapple'과 더 가까워야 한다는 걸 직감적으로 알 수 있음.
워드 임베딩은 여러 문장을 학습하면서 배운 문맥으로 벡터 간의 거리를 조정해줌. 그래서 'apple'과 'pineapple'의 벡터 간 거리를 가깝게 해주고, 'a'와 'an'의 거리를 가깝게 해 주지만 'apple'과 'a'의 거리는 멀어지도록 함.
이런 특성으로 인해 다른 NLU task를 수행할 때 이점이 생김. 예를 들어 카페 주문을 받는 챗봇이 있다고 할 때, 'I'll order a glass of apple juice.'라는 문장의 의도를 '주문'이라고 학습시켰다면 'I'll order a glass of pineapple juice.'라는 문장은 학습시키지 않더라도 '주문'에 해당하는 문장이라고 인식할 확률이 높아짐.

#### 2. 벡터간 의미를 가진 수리적 연산(사칙연산 등)이 가능해진다.
one-hot 벡터도 벡터의 형태이므로 벡터 간 더하고 빼는 등의 수리 연산은 가능함. 하지만 연산의 결과가 아무 의미가 없음
그러나 워드 임베딩에서는 벡터 간의 덧셈, 뺄셈이 의미를 지님 예를 들어 '삼성전자'라는 단어 벡터에서 '삼성'이라는 단어 벡터의 값을 빼면 무언가 전자기기를 다루는 회사에 대한 특성이 벡터로 표현되어 있을 것. 여기에 'LG'라는 단어 벡터의 값을 더해보면 '삼성전자-삼성+LG'연산의 결과 벡터와 가장 근접한 단어 벡터를 찾으면 'LG전자'가 나타날 것. 이렇듯 수리 연산에서 각 단어의 의미 특징을 살리는 워드 임베딩의 특징으로 인해, 대부분의 딥러닝 기반 NLU 과제에서는 워드 임베딩을 적용했을 때 정확도가 더 높아지곤 함.

### CBOW
CBOW(Continuous Bag-of-Words)는 문장 내에서 주변 단어들을 모델에게 제공하고, 가운데 빈칸에 올 단어가 무엇인지 맞추는 방식으로 워드 임베딩을 학슴함. k개만큼의 주변 단어가 주어지면 중심에 올 단어의 조건부 확률을 계산하는 방식.
예를 들어 '원숭이에게 (A)를 주었더니 껍질도 안 벗기고 잘 먹더라.'라는 문장이 있을 때, 학습되지 않은 초기 모형은 A에 들어갈 모든 단어의 확률이 동일하다고 여김

p(A=바나나) = p(A=귤) = p(A=비행기) = p(A=자동차) = ...

하지만 다양한 문장들로 모형을 학습시킬수록 모델은 의미 있는 단어를 뽑아낼 수 있음

p(A=바나나) > p(A=귤) > ... >> ... > p(A=비행기) > p(A=자동차) > ...

이러한 과정들을 반복하면서 모델은 단어의 특성을 학습하게 되며, 좋은 품질의 워드 임베딩 벡터를 만들게 됨

### SKIP-GRAM
SKIP-GRAM은 한 단어를 모델에 제공하면, 모델은 이 주변에 어떤 단어들이 놓일지 맞히는 방식으로 학습함. 주어진 중심 단어 주변의 k개 단어가 어떤 것이 나타날지 전부 확률을 계산하는 방식임. 따라서 하나의 빈칸만 맞추면 되었던 CBOW에 비해 k개의 단어나 맞춰야 하는 skip-gram 모델은 당연히 더 어려운 과제를 수행해야 함. 이로 인해 CBOW보다 상대적으로 더 많은 학습 데이터가 필요하며, 더 어려운 과제를 수행해야 하므로 결과적으로 워드 임베딩의 품질은 더 좋은 경향이 있음
Skip-gram의 예를 들자면, '(A)부터 (B)까지.'라는 문장이 있을 때 학습되지 않은 초기 모형은 다양한(말이 안 되는) A, B의 단어 조합을 출력할 것임. 하지만 다양한 문장을 모형에 학습시킬수록 p(A=머리)의 확률이 높아지는 등의 추론이 가능해짐




## 참고자료
1. LG CNS 블로그-인간의 언어를 이해하는 기계, NLU : https://blog.lgcns.com/1672?category=515093
2. LG CNS 블로그-인간의 언어를 이해하는 기계, NLU에는 어떤 것이 있을까? : https://blog.lgcns.com/1680?category=515093



