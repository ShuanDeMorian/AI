{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_story_file(text_file):\n",
    "    with open(text_file, \"r\",encoding='utf-8') as f:\n",
    "        # sentences are separated by 2 newlines\n",
    "        # single newlines might be image captions\n",
    "        # so will be incomplete sentence\n",
    "        lines = f.read().split('\\n\\n')\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"my_data/stories_tokenized/00a39c134080b6f215a81c15d46c3ac7cc7bdcf3.story\"\n",
    "a=read_story_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ATLANTA, Georgia (CNN)  -- Dressed head to toe in black, designer Isaac Mizrahi is wearing an outfit that seems to contradict his personality -- and his usual fashion flair.',\n",
       " 'Isaac Mizrahi has earned four awards from the Council of Fashion Designers of America.',\n",
       " '\"I always start with color when I\\'m designing things. Always. If the color is right, I feel better,\" he touts on his Web site.',\n",
       " 'But Mizrahi has an explanation for his less than colorful attire on a recent visit to CNN Center in Atlanta.',\n",
       " '\"We have this very quick trip, and we have to go right back and there\\'s no time to pack and we can\\'t check luggage ... so I focused it to black, gray and white.\"',\n",
       " 'It\\'s just one more style tip you can pick up from Mizrahi\\'s new book \"How to Have Style\" (Gotham). Despite the slightly audacious title, Mizrahi, who has won four awards from the Council of Fashion Designers of America, has earned the right to tell women how to dress.',\n",
       " 'For five years, the New York fashion designer has been selling low-priced clothing and home furnishings at Target. But with his new book comes a new job -- as the creative director of Liz Claiborne.',\n",
       " 'CNN talked to Mizrahi about his love for theater, the most common fashion mistakes and why bad flowers are never OK. The following is an edited version of that interview:',\n",
       " 'CNN: You started in acting at the High School of the Performing Arts. How do you combine that love and your love for design?',\n",
       " \"Isaac Mizrahi: Well, you know what, I think it's all theater. I think that fashion is a form of entertainment. And I think that these days, as a fashion designer, it's almost like you represent a political party or something.\",\n",
       " 'Like women say, \"Oh, that\\'s a brand name I associate with because I\\'ve worn it before. I love it. It seems to fulfill who I am really easily.\" Whatever it is, she knows that it just makes her life really easy so she associates with it, you know? And in the end, I am like this personality that represents that.  Watch Mizrahi talk about his new book ¬ª',\n",
       " 'But more than that, I have designs [in] the entertainment business. There\\'s a movie called \"Unzipped\" about me that was a really successful movie. I had two TV series. I design costumes constantly for theater and ballet and opera. So to me it\\'s all one big world. It\\'s seamless to me.',\n",
       " \"CNN: What's your daily schedule like? It has to be crazy with all that you do.\",\n",
       " \"Mizrahi: It changes every day, and I really like that. There's a base to it. I wake up, I go swimming every day, and I eat the same breakfast almost every single day. But when I get to work is when it changes up.\",\n",
       " 'Some days I work in the showroom; some days I work in the design room; some days I actually work in my own private studio, where I just do sketches and sketches and sketches. Other days I work in the TV studio taping segments and my Web show.',\n",
       " \"I don't really love travel. I feel like it really disrupts what I love doing most, which is this creation, you know what I mean? When I finally let myself enjoy it, I can enjoy traveling. But it takes a great agony for me to separate from New York City and my studios and the people that I work with.\",\n",
       " \"CNN: Say you're walking down the street. What's the most common style error that you see in people?\",\n",
       " \"Mizrahi: I see a lot, a lot, a lot of bad hair. I would say that's the most common style error I see is bad hair.\",\n",
       " \"You know people have excuses for bad shoes -- because you know some people have back problems, it is the street and they're walking and walking and walking -- but I do think that people have no excuse for bad hair. Because you know what? There's a hat, if your hair is really that bad that day.\",\n",
       " \"But I always think that women should be encouraged to spend a lot of money on their hair. It's like you should spend your most money ... on your hair. You'd think I had a chain of hair salons, but I don't. [Laughter].\",\n",
       " \"CNN: What about in home furnishing? You do a line for Target that's ending this year. What's the most common mistake people make there?\",\n",
       " \"Mizrahi: You know what it is with people? I think people get lazy when it comes to being at home -- they leave things around. I like to think about cabinets. I like to put things away as much as possible. It's like salt shakers on the table? No. You put the salt shakers in a cabinet, and the table looks so much better when it's plain. You know what I mean?\",\n",
       " \"And people just think that bad flowers are better than nothing, but I disagree with that. I think that nothing is way better than bad flowers. You either have gorgeous, gorgeous flowers, or you have no flowers. Like at a dinner party, I prefer no flowers usually to the flowers that people have on the table. That's awful, but it's true.\",\n",
       " 'CNN: How has your personal style evolved over time?',\n",
       " \"Mizrahi: It's gotten a lot quieter, my personal style. I used to dress, dress, dress, dress, dress, and I don't know, I dress in a very particular way now and it's almost like clockwork. And every once in a while I break out and do something crazy.\",\n",
       " 'CNN: Can you describe your personality for me, and how it affects your style?',\n",
       " \"Mizrahi: I don't know. It's very hard to describe one's personality. I can't say about my personality, but I like to think that I'm very exposed to what's going on out there in the world culturally, and that's what influences my design.\",\n",
       " \"It's kind of like here's the 360 degrees of what's going on [in] the world culturally, you know? Socioeconomically, culturally, and here's my response to it. Here's what the clothes look like; here's what you should be wearing. And it's kind of like a wonderful edge, you're standing, and yet there's room enough in there for your own interpretation or to move in one direction or another.\",\n",
       " \"Oh! Here's a good description of my personality: claustrophobic. I am very claustrophobic. I don't like to commit to one thing necessarily, but when I do commit to it, it's whole and complete.\",\n",
       " '@highlight',\n",
       " 'Designer Isaac Mizrahi moving from Target to creative director at Liz Claiborne',\n",
       " '@highlight',\n",
       " 'Mizrahi says he believes \"bad hair\" is the most common style mistake  Designer says he prefers no flowers to \"bad\" ones',\n",
       " '@highlight',\n",
       " 'Mizrahi admits he has a \"claustrophobic\" personality']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÎπÑÎîîÏò§ ÏïÑÏù¥ÎîîÏôÄ ÏûêÎßâ story_tokenizedÎ°ú ÎßåÎì§Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "dataPath=\"my_data/videoId\"\n",
    "targetPath=\"my_data/stories_tokenized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(filename):\n",
    "    file = dataPath + '/' + filename + '.json'\n",
    "    with open(file) as f:\n",
    "        json_data = json.load(f)\n",
    "    comments = []\n",
    "    for c in json_data['comments']:\n",
    "        comments += [c['textOriginal']]\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for file in os.listdir(dataPath):\n",
    "    if file.endswith(\".txt\"):\n",
    "        filename = file[:-4]\n",
    "        file = dataPath + '/' + file\n",
    "        data= open(file,'r',encoding='utf-8').read()\n",
    "        comments = get_comments(filename)\n",
    "        i = 0\n",
    "        for c in comments:\n",
    "            i += 1\n",
    "            temp = data\n",
    "            temp += '\\n\\n' + '@highlight'\n",
    "            temp += '\\n\\n' + c\n",
    "            save_file = targetPath + '/' + filename + '_sample'+ str(i) + '.story'\n",
    "            f = open(save_file,'w',encoding='utf-8')\n",
    "            f.write(temp)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ugh i love how girly she is her aesthetic is just wow üíóüíÖüèª',\n",
       " 'This video is so lovely! Makes me want to travel to Paris even more! ‚ò∫Ô∏èüíï‚ú®',\n",
       " '‚òÅÔ∏èüíïTrust me Catherine your channel will grow by fast!! Your video are so lovely ! Keep it up ! üíï‚òÅÔ∏è',\n",
       " \"Omg! I literally gasped when I saw this video! The pink! Aesthetic goals üíùüíï I can't wait until my trip to Paris in February.\",\n",
       " 'Your vlog was adorable. As a Parisian I have to admit I loved seeing the city through your eyes, sometimes when you see it everyday you forget the nice parts of it ! I hope to see more vlogs and/or videos of you in the future <3',\n",
       " 'I love your style üòç please do hauls or lookbooks, it would be amazing üíñ',\n",
       " 'Film a room tour and a wardrobe tour, too, please <3 Luv ya.',\n",
       " 'Dear Catherine, \\nI really loved this vlog. I really like Paris and you did a great job on catching the beauty of it. Please do continue your vlogs. \\nKisses from Germany,S.',\n",
       " \"We are sooooo üòähappy you were ‚ú®brave. It's a lovely hat on you. And this was a delightful vlog ü§ó\",\n",
       " 'I just got back from paris and i went because of this video and it has been Fabulous!!! Thank you!',\n",
       " 'What camera do you use to vlog with? xx',\n",
       " 'Omg where do you get your background music from?! It‚Äôs so beautiful',\n",
       " 'You look beautiful even though you were not well enough. Love the tea room & your pink Barrett. Going to london - Paris next month .cant wait! Thx for the vlog',\n",
       " 'What a magical, airy, bright and fun experience. Love it. Thank you for sharing.',\n",
       " 'can you do this makeup tutorial? <3',\n",
       " 'catherine I simply ADORED this!!‚ô°',\n",
       " \"It's like I've been to Paris with you watchin' this video! Cute details and style! Great job!üòçüòçüòç\",\n",
       " 'This was some really good, fun experimental filmmaking... loved the explorations with focus in and out, textured shots, a wonderful, artistic taste of Paris, thank you! You put me right there. And the tour of the tea shop with the packaging... great for me as I enjoy growing my own tea herbs. The focus on packaging was a lesson too. Nice work! Keep it up!',\n",
       " 'And I hope you feel better!! üíïüôèüèº',\n",
       " \"Hello! What a great video I love your taste. I also love the beret it's just cherry. I'm a new subscriber my name is Willow Sage\",\n",
       " 'Can you please do a makeup tutorial? Your makeup looks so pretty',\n",
       " 'Love all your videos! Would love to see some outfit videos/clothing hauls, you have such lovely style! Xxx',\n",
       " 'HOLA BONITA! I‚Äôm SO in love with this video and the editing üíå check out my Paris vlog‚ú®',\n",
       " \"I have just found ur channel n blogüòçüòçüòç... N i feel like I've found treasure ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è\",\n",
       " 'Soo prettyüòçüòç',\n",
       " \"Love your videos so much! I would love to see a hair tutorial on your hair at 5:00 minutes it's so pretty! Also would love a girly lookbook, you have such a cute and girly style! xx\",\n",
       " 'Women should always have their own sense of style and not worry about being embarrassed - wear what you want and feel good in .  It shows confidence.',\n",
       " \"This vlog is super cute,super pretty. Love the camera work.  I visited laduree. Those macaroons are sooooo yummy. Pricy, but man are they tasty. Omg.  ü§óü§óü§ó Can't wait to visit the one on the Champs Elysee in gorgeous Paris soon. Such a stunningly beautiful city.\",\n",
       " 'I really enjoyed your video! Your camera work is lovely, and you showed different things to the usual Paris vlogs. Loved it! üíñ',\n",
       " \"Brave or not...you do embody the ultimate tourist Paris clich√©...but hey, one cannot argue about taste. I hope you've seen more facets of Paris than just the clich√© postcard image.\",\n",
       " \"THIS IS THE BEST PARIS VLOG I'VE SEEN\",\n",
       " 'I love your vlogs girl !!!‚ù§‚ù§‚ù§‚ù§‚ù§',\n",
       " 'Ladureeüòç did you get any of their cosmetics by the way ?i love its packaging n pink color!!',\n",
       " 'I wanted to say that you looked lovely on your pink pearly beret.\\nYou should never feel like an idiot to wear it.\\nNice vid.',\n",
       " 'Hi! I am your follower from Instagram. Love your profile',\n",
       " 'Hi Catherine, I just came across you through your Paris Vlog. Love it, thank you for showing us your trip üá´üá∑ N ur a beautiful gal ‚ù§Ô∏è',\n",
       " 'üíãThis place is beautiful üôèüë∏üèªüíã',\n",
       " 'Your hair color is beyond beautiful',\n",
       " 'Would love to see whole outfit pics! Cute channel',\n",
       " 'I love that beret! So pretty! I wish I was in Paris right now!:)',\n",
       " 'I have watched countless Paris vlogs and never heard of the Marie Antoinette tea room. I love her and am so glad I know about it now!!! Thank you so much! Loved the whole vlog, but this was the most  exciting part for me,',\n",
       " 'Great video Catherine. You captured even the tiniest details. Please what music did you have playing at 09.32',\n",
       " 'This video is so pretty. Thanks so much!',\n",
       " 'Really finishing this parisian vlog with a british perfume!!!!',\n",
       " 'I don‚Äôt get tired watching your video over and over. This may be your first vlog but it‚Äôs sooo amazing! I‚Äôd like to visit Paris one day too.. you make things look so pretty all the time. You even look pretty when you‚Äôre sick.. please make more videos! Thank you for sharing your trip with us!! Hope you feel better now..',\n",
       " \"I just found your instagram and youtube channel, and I'm in love with all your videos! <3 Please make more of them!\",\n",
       " 'Love your video. Paris haul please?',\n",
       " 'Oh my gosh just gorgeous! Everything!',\n",
       " 'You look adorable in that hat :-) Glad you wore it!!!!!!!! Sorry you were sick. Thanks for taking us along.  That was sweet of you to think of us when you were feeling bad. Hugs.',\n",
       " \"Where do you find this music? It's so cute! Loved the video! üéÄüíõ\",\n",
       " 'beautiful ‚ô°',\n",
       " 'soooo pretty! I love this video',\n",
       " 'omg I love this <3 and pinky which I love I hope to travel soon xx',\n",
       " 'Love everything about this video! I love Paris, love the pink aesthetic and you are so pretty <3',\n",
       " 'Great video!! And you are very pretty!!!',\n",
       " 'omg i love your videos so much!! what camera do you use? üíï',\n",
       " 'Sooo beautiful üòçüòçüòçüòçyou‚Äôve highlighted Paris in the best way possible! Amazing vlog!',\n",
       " \"This video was so beautiful! I'm happy you enjoyed Paris! You look gorgeous too ‚ô•\",\n",
       " 'The style of this vlog is just AMAZING üòç üòò',\n",
       " 'i wanna macaroon now',\n",
       " 'I loved this vlog, please do more and you do your makeup so beautifullyüí≠üéÄ',\n",
       " 'where is the purse from <3?',\n",
       " 'Love love love the beret!üíï',\n",
       " 'Love this video! So lovely :)',\n",
       " 'Hello. I really enjoyed watching your video! It‚Äôs as if I was there with you enjoying all the good food! üòã The Marie Antoinette tea is so cute and I love the packaging! The Eiffel Tower looked great at night! üòç I like your music also! I ‚ù§Ô∏è LADUREE and actually have Laduree charms unboxing on my channel! ü§ó I have the same my Burberry perfume also. üëç TFS. New subbie.',\n",
       " 'I love paris it is full of pretty colors',\n",
       " 'Wow such an aesthetically pleasing video! Can I know what kind of camera are you using? The focus and bokeh during the Eiffel Tower was amazing!',\n",
       " 'Any one want to suscribe each other',\n",
       " 'Please post more video.',\n",
       " 'This is exactly what I wanted to watch, I live on the other side of the world and sooo wanna visit Paris. I love your inclusion of all the little details, it makes it so much more \"real\" for the viewer than just the big shots of the skyline. Love your aesthetic and choice of music and just EVERYTHING. Thank you so much, I hope you\\'re feeling much better.',\n",
       " 'I been following you on Instagram for while I was so happy that you finally doing a YouTube, let help Catherine get a YouTube plaque she amazing x',\n",
       " 'Um, wow. Such a beautiful thumbnail and video. üò≥üòçüòçüíï',\n",
       " 'More videos please!!!üíñüíüüíñ',\n",
       " \"Ah I loved this vlog and your trip to Paris looked lovely! Such a shame that you weren't feeling great - it always happens like that doesn't it when you go away?! The Burberry perfume looks beautiful too xo\",\n",
       " 'Just came across your channel and subscribed immediately after seeing your video. Loved the beret on you! Was your pink bag Kate spade? Could you link your bag? Loved it!!',\n",
       " 'What a gorgeous video! I look forward to all of your videos and your beautiful pictures on instagram! And your eyeliner is sooo pretty! I hope you feel better and I am anxiously awaiting your next video, xoxo',\n",
       " 'I love you sooooooooo much . You are soooo√≤oo√≤o cute',\n",
       " 'love your vlog!!!!! want more!!!!!! :)))))',\n",
       " \"Just wanted to say I've been following you on instagram for a while now and I was delighted when I found out that you had started your YouTube channel üíï loved the vlog, Paris is such a pretty and elegant city just like you it would seem xx \\n(Ps- loving the beret, it doesn't look silly at all)\",\n",
       " 'Great video. Loved the beret.',\n",
       " '‚ù§Ô∏èüëåüôè',\n",
       " '‚ù§Ô∏èüôèüòòüá∫üá∏',\n",
       " \"Awe! this vlog is so lovely! It's my dream to go to Paris! Thank you so much for filming your trip to Paris Love! The Eiffel Tower at night is just so beautiful! & Feel better soon! :) xx\",\n",
       " '–ü–æ–¥—Å–∫–∞–∂–∏ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞ –Ω–∞ –∫–∞–∫—É—é –∫–∞–º–µ—Ä—É —Å–Ω–∏–º–∞–µ—à—å ?',\n",
       " 'love this video! makes me miss paris! x',\n",
       " 'Hello! Please, help meüòÖ I‚Äôm currently planning a trip to Paris and after your video I fell in love with Ladur√©e! What is the address of the one you have visited?',\n",
       " 'Paris is still beautiful the Chinese are destroying New York with their ugly high rises. It saddens me.\\nThey put ugly high rises between beautiful historic buildings. \\nThey don‚Äôt care. \\nGeez it‚Äôs sad you‚Äôre sick I‚Äôm glad you can still enjoy Paris it‚Äôs a magical place.\\nThe trip of a lifetime. üòä']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] ÏßÄÏ†ïÎêú Í≤ΩÎ°úÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-5519df321795>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m   \u001b[1;31m# Check the stories directories contain the correct number of .story files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m   \u001b[0mcheck_num_stories\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn_stories_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_expected_cnn_stories\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m   \u001b[0mcheck_num_stories\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdm_stories_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_expected_dm_stories\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-64-5519df321795>\u001b[0m in \u001b[0;36mcheck_num_stories\u001b[1;34m(stories_dir, num_expected)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcheck_num_stories\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstories_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_expected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m   \u001b[0mnum_stories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstories_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnum_stories\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnum_expected\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"stories directory %s contains %i files but should contain %i\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstories_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_stories\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_expected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] ÏßÄÏ†ïÎêú Í≤ΩÎ°úÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: '-f'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import hashlib\n",
    "import struct\n",
    "import subprocess\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "\n",
    "dm_single_close_quote = u'\\u2019' # unicode\n",
    "dm_double_close_quote = u'\\u201d'\n",
    "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"', dm_single_close_quote, dm_double_close_quote, \")\"] # acceptable ways to end a sentence\n",
    "\n",
    "# We use these to separate the summary sentences in the .bin datafiles\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "all_train_urls = \"url_lists/all_train.txt\"\n",
    "all_val_urls = \"url_lists/all_val.txt\"\n",
    "all_test_urls = \"url_lists/all_test.txt\"\n",
    "\n",
    "cnn_tokenized_stories_dir = \"cnn_stories_tokenized\"\n",
    "dm_tokenized_stories_dir = \"dm_stories_tokenized\"\n",
    "finished_files_dir = \"finished_files\"\n",
    "chunks_dir = os.path.join(finished_files_dir, \"chunked\")\n",
    "\n",
    "# These are the number of .story files we expect there to be in cnn_stories_dir and dm_stories_dir\n",
    "num_expected_cnn_stories = 92579\n",
    "num_expected_dm_stories = 219506\n",
    "\n",
    "VOCAB_SIZE = 200000\n",
    "CHUNK_SIZE = 1000 # num examples per chunk, for the chunked data\n",
    "\n",
    "\n",
    "def chunk_file(set_name):\n",
    "  in_file = 'finished_files/%s.bin' % set_name\n",
    "  reader = open(in_file, \"rb\")\n",
    "  chunk = 0\n",
    "  finished = False\n",
    "  while not finished:\n",
    "    chunk_fname = os.path.join(chunks_dir, '%s_%03d.bin' % (set_name, chunk)) # new chunk\n",
    "    with open(chunk_fname, 'wb') as writer:\n",
    "      for _ in range(CHUNK_SIZE):\n",
    "        len_bytes = reader.read(8)\n",
    "        if not len_bytes:\n",
    "          finished = True\n",
    "          break\n",
    "        str_len = struct.unpack('q', len_bytes)[0]\n",
    "        example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "        writer.write(struct.pack('q', str_len))\n",
    "        writer.write(struct.pack('%ds' % str_len, example_str))\n",
    "      chunk += 1\n",
    "\n",
    "\n",
    "def chunk_all():\n",
    "  # Make a dir to hold the chunks\n",
    "  if not os.path.isdir(chunks_dir):\n",
    "    os.mkdir(chunks_dir)\n",
    "  # Chunk the data\n",
    "  for set_name in ['train', 'val', 'test']:\n",
    "    print(\"Splitting %s data into chunks...\" % set_name)\n",
    "    chunk_file(set_name)\n",
    "  print(\"Saved chunked data in %s\" % chunks_dir)\n",
    "\n",
    "\n",
    "def tokenize_stories(stories_dir, tokenized_stories_dir):\n",
    "  \"\"\"Maps a whole directory of .story files to a tokenized version using Stanford CoreNLP Tokenizer\"\"\"\n",
    "  print(\"Preparing to tokenize %s to %s...\" % (stories_dir, tokenized_stories_dir))\n",
    "  stories = os.listdir(stories_dir)\n",
    "  # make IO list file\n",
    "  print(\"Making list of files to tokenize...\")\n",
    "  with open(\"mapping.txt\", \"w\") as f:\n",
    "    for s in stories:\n",
    "      f.write(\"%s \\t %s\\n\" % (os.path.join(stories_dir, s), os.path.join(tokenized_stories_dir, s)))\n",
    "  command = ['java', 'edu.stanford.nlp.process.PTBTokenizer', '-ioFileList', '-preserveLines', 'mapping.txt']\n",
    "  print(\"Tokenizing %i files in %s and saving in %s...\" % (len(stories), stories_dir, tokenized_stories_dir))\n",
    "  subprocess.call(command)\n",
    "  print(\"Stanford CoreNLP Tokenizer has finished.\")\n",
    "  os.remove(\"mapping.txt\")\n",
    "\n",
    "  # Check that the tokenized stories directory contains the same number of files as the original directory\n",
    "  num_orig = len(os.listdir(stories_dir))\n",
    "  num_tokenized = len(os.listdir(tokenized_stories_dir))\n",
    "  if num_orig != num_tokenized:\n",
    "    raise Exception(\"The tokenized stories directory %s contains %i files, but it should contain the same number as %s (which has %i files). Was there an error during tokenization?\" % (tokenized_stories_dir, num_tokenized, stories_dir, num_orig))\n",
    "  print(\"Successfully finished tokenizing %s to %s.\\n\" % (stories_dir, tokenized_stories_dir))\n",
    "\n",
    "\n",
    "def read_text_file(text_file):\n",
    "  lines = []\n",
    "  with open(text_file, \"r\") as f:\n",
    "    for line in f:\n",
    "      lines.append(line.strip())\n",
    "  return lines\n",
    "\n",
    "\n",
    "def hashhex(s):\n",
    "  \"\"\"Returns a heximal formated SHA1 hash of the input string.\"\"\"\n",
    "  h = hashlib.sha1()\n",
    "  h.update(s.encode())\n",
    "  return h.hexdigest()\n",
    "\n",
    "\n",
    "def get_url_hashes(url_list):\n",
    "  return [hashhex(url) for url in url_list]\n",
    "\n",
    "\n",
    "def fix_missing_period(line):\n",
    "  \"\"\"Adds a period to a line that is missing a period\"\"\"\n",
    "  if \"@highlight\" in line: return line\n",
    "  if line==\"\": return line\n",
    "  if line[-1] in END_TOKENS: return line\n",
    "  # print line[-1]\n",
    "  return line + \" .\"\n",
    "\n",
    "\n",
    "def get_art_abs(story_file):\n",
    "  lines = read_text_file(story_file)\n",
    "\n",
    "  # Lowercase everything\n",
    "  lines = [line.lower() for line in lines]\n",
    "\n",
    "  # Put periods on the ends of lines that are missing them (this is a problem in the dataset because many image captions don't end in periods; consequently they end up in the body of the article as run-on sentences)\n",
    "  lines = [fix_missing_period(line) for line in lines]\n",
    "\n",
    "  # Separate out article and abstract sentences\n",
    "  article_lines = []\n",
    "  highlights = []\n",
    "  next_is_highlight = False\n",
    "  for idx,line in enumerate(lines):\n",
    "    if line == \"\":\n",
    "      continue # empty line\n",
    "    elif line.startswith(\"@highlight\"):\n",
    "      next_is_highlight = True\n",
    "    elif next_is_highlight:\n",
    "      highlights.append(line)\n",
    "    else:\n",
    "      article_lines.append(line)\n",
    "\n",
    "  # Make article into a single string\n",
    "  article = ' '.join(article_lines)\n",
    "\n",
    "  # Make abstract into a signle string, putting <s> and </s> tags around the sentences\n",
    "  abstract = ' '.join([\"%s %s %s\" % (SENTENCE_START, sent, SENTENCE_END) for sent in highlights])\n",
    "\n",
    "  return article, abstract\n",
    "\n",
    "\n",
    "def write_to_bin(url_file, out_file, makevocab=False):\n",
    "  \"\"\"Reads the tokenized .story files corresponding to the urls listed in the url_file and writes them to a out_file.\"\"\"\n",
    "  print(\"Making bin file for URLs listed in %s...\" % url_file)\n",
    "  url_list = read_text_file(url_file)\n",
    "  url_hashes = get_url_hashes(url_list)\n",
    "  story_fnames = [s+\".story\" for s in url_hashes]\n",
    "  num_stories = len(story_fnames)\n",
    "\n",
    "  if makevocab:\n",
    "    vocab_counter = collections.Counter()\n",
    "\n",
    "  with open(out_file, 'wb') as writer:\n",
    "    for idx,s in enumerate(story_fnames):\n",
    "      if idx % 1000 == 0:\n",
    "        print(\"Writing story %i of %i; %.2f percent done\" % (idx, num_stories, float(idx)*100.0/float(num_stories)))\n",
    "\n",
    "      # Look in the tokenized story dirs to find the .story file corresponding to this url\n",
    "      if os.path.isfile(os.path.join(cnn_tokenized_stories_dir, s)):\n",
    "        story_file = os.path.join(cnn_tokenized_stories_dir, s)\n",
    "      elif os.path.isfile(os.path.join(dm_tokenized_stories_dir, s)):\n",
    "        story_file = os.path.join(dm_tokenized_stories_dir, s)\n",
    "      else:\n",
    "        print(\"Error: Couldn't find tokenized story file %s in either tokenized story directories %s and %s. Was there an error during tokenization?\" % (s, cnn_tokenized_stories_dir, dm_tokenized_stories_dir))\n",
    "        # Check again if tokenized stories directories contain correct number of files\n",
    "        print(\"Checking that the tokenized stories directories %s and %s contain correct number of files...\" % (cnn_tokenized_stories_dir, dm_tokenized_stories_dir))\n",
    "        check_num_stories(cnn_tokenized_stories_dir, num_expected_cnn_stories)\n",
    "        check_num_stories(dm_tokenized_stories_dir, num_expected_dm_stories)\n",
    "        raise Exception(\"Tokenized stories directories %s and %s contain correct number of files but story file %s found in neither.\" % (cnn_tokenized_stories_dir, dm_tokenized_stories_dir, s))\n",
    "\n",
    "      # Get the strings to write to .bin file\n",
    "      article, abstract = get_art_abs(story_file)\n",
    "\n",
    "      # Write to tf.Example\n",
    "      tf_example = example_pb2.Example()\n",
    "      tf_example.features.feature['article'].bytes_list.value.extend([article.encode()])\n",
    "      tf_example.features.feature['abstract'].bytes_list.value.extend([abstract.encode()])\n",
    "      tf_example_str = tf_example.SerializeToString()\n",
    "      str_len = len(tf_example_str)\n",
    "      writer.write(struct.pack('q', str_len))\n",
    "      writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "\n",
    "      # Write the vocab to file, if applicable\n",
    "      if makevocab:\n",
    "        art_tokens = article.split(' ')\n",
    "        abs_tokens = abstract.split(' ')\n",
    "        abs_tokens = [t for t in abs_tokens if t not in [SENTENCE_START, SENTENCE_END]] # remove these tags from vocab\n",
    "        tokens = art_tokens + abs_tokens\n",
    "        tokens = [t.strip() for t in tokens] # strip\n",
    "        tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
    "        vocab_counter.update(tokens)\n",
    "\n",
    "  print(\"Finished writing file %s\\n\" % out_file)\n",
    "\n",
    "  # write vocab to file\n",
    "  if makevocab:\n",
    "    print(\"Writing vocab file...\")\n",
    "    with open(os.path.join(finished_files_dir, \"vocab\"), 'w') as writer:\n",
    "      for word, count in vocab_counter.most_common(VOCAB_SIZE):\n",
    "        writer.write(word + ' ' + str(count) + '\\n')\n",
    "    print(\"Finished writing vocab file\")\n",
    "\n",
    "\n",
    "def check_num_stories(stories_dir, num_expected):\n",
    "  num_stories = len(os.listdir(stories_dir))\n",
    "  if num_stories != num_expected:\n",
    "    raise Exception(\"stories directory %s contains %i files but should contain %i\" % (stories_dir, num_stories, num_expected))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  if len(sys.argv) != 3:\n",
    "    print(\"USAGE: python make_datafiles.py <cnn_stories_dir> <dailymail_stories_dir>\")\n",
    "    sys.exit()\n",
    "  cnn_stories_dir = sys.argv[1]\n",
    "  dm_stories_dir = sys.argv[2]\n",
    "\n",
    "  # Check the stories directories contain the correct number of .story files\n",
    "  check_num_stories(cnn_stories_dir, num_expected_cnn_stories)\n",
    "  check_num_stories(dm_stories_dir, num_expected_dm_stories)\n",
    "\n",
    "  # Create some new directories\n",
    "  if not os.path.exists(cnn_tokenized_stories_dir): os.makedirs(cnn_tokenized_stories_dir)\n",
    "  if not os.path.exists(dm_tokenized_stories_dir): os.makedirs(dm_tokenized_stories_dir)\n",
    "  if not os.path.exists(finished_files_dir): os.makedirs(finished_files_dir)\n",
    "\n",
    "  # Run stanford tokenizer on both stories dirs, outputting to tokenized stories directories\n",
    "  tokenize_stories(cnn_stories_dir, cnn_tokenized_stories_dir)\n",
    "  tokenize_stories(dm_stories_dir, dm_tokenized_stories_dir)\n",
    "\n",
    "  # Read the tokenized stories, do a little postprocessing then write to bin files\n",
    "  write_to_bin(all_test_urls, os.path.join(finished_files_dir, \"test.bin\"))\n",
    "  write_to_bin(all_val_urls, os.path.join(finished_files_dir, \"val.bin\"))\n",
    "  write_to_bin(all_train_urls, os.path.join(finished_files_dir, \"train.bin\"), makevocab=True)\n",
    "\n",
    "  # Chunk the data. This splits each of train.bin, val.bin and test.bin into smaller chunks, each containing e.g. 1000 examples, and saves them in finished_files/chunks\n",
    "  chunk_all()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
