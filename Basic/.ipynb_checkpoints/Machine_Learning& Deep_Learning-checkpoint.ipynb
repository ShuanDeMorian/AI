{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "머신 러닝의 간단한 설명   \n",
    "출처 : 책(케라스 창시자에게 배우는 딥러닝)\n",
    "\n",
    "\n",
    "딥러닝은 머신 러닝의 넓은 범주 안으로 인식한다   \n",
    "\n",
    "\n",
    "**확률적 모델링(probabilistic modeling)** 은 통계학 이론을 데이터 분석에 응요한 것입니다. 초창기 머신 러닝 형태 중 하나고 요즘도 널리 사용됩니다. 가장 잘 알려진 알고리즘 중 하나는 나이브 베이즈(Naibe Bayes) 알고리즘입니다.   \n",
    "\n",
    "\n",
    "나이브 베이즈는 입력 데이터의 특성이 모두 독립적이라고 가정하고 베이즈 정리(Bayes' theorem)를 적용하는 머신 러닝 분류 알고리즘입니다. (강한 또는 '순진한'(naive) 가정입니다. 여기에서 이름이 유래되었습니다.) 이런 형태의 데이터 분석은 컴퓨터보다 앞서 있었기 때문에 첫 번째 컴퓨터가 등장하기 수십년 전에는 수작업으로 적용했습니다(거의 1950년대로 거슬러 올라갑니다.) 베이즈 정리와 통계의 토대는 18세기까지 거슬러 올라갑니다. 이 정도가 나이브 베이즈 분류기를 사용하기 위해 알아야 할 전부입니다.\n",
    "\n",
    "이와 밀접하게 연관된 모델이 **로지스틱 회귀(logistic regression)** 입니다(줄여서 logreg라고 하겠습니다). 이 모델은 현대 머신 러닝의 \"Hello world\"로 여겨집니다. 이름 떄문에 혼동하지 마세요. logreg는 회귀(regression) 알고리즘이 아니라 분류(classification) 알고리즘입니다. 나이브 베이즈와 매우 비슷하게 logreg는 컴퓨터보다 훨씬 오래 전부터 있었습니다. 하지만 간단하고 다목적으로 활용할 수 있어서 오늘날에도 여전히 유용합니다.   \n",
    "데이터 과학자가 분류 작업에 대한 감을 빠르게 얻기 위해 데이터셋에 적용할 첫 번째 알고리즘으로 선택하는 경우가 많습니다. \n",
    "\n",
    "\n",
    "(중략) **SVM**에 대한 설명\n",
    "\n",
    "분류 문제를 간단하게 만들어 주기 위해 데이터를 고차원 표현으로 매핑하는 기법이 이론상으로는 좋아 보이지만 실제로는 컴퓨터로 구현하기 어려운 경우가 많습니다. 그래서 **커널 기법(kernel trick)** 이 등장했습니다(커널 방법의 핵심 아이디어로 여기에서 이름을 따왔습니다). 요지는 다음과 같습니다. 새롭게 표현된 공간에서 좋은 결정 초평면을 찾기 위해 새로운 공간에 대응하는 데이터 포인트의 좌표를 실제로 구할 필요가 없습니다. **새로운 공간에서의 두 데이터 포인트 사이의 거리를 계산할 수만 있으면 됩니다.** **커널 함수(kernel function)**를 사용하면 이를 효율적으로 계산할 수 있습니다. 커널 함수는 원본 공간에 있는 두 데이터 포인트를 명시적으로 새로운 표현으로 변환하지 않고 타깃 표현 공간에 위치했을 때의 거리를 매핑해주는 계산 가능한 연산입니다. 커널 함수는 일반적으로 데이터로부터 학습되지 않고 직접 만들어야 합니다. SVM에서 학습되는 것은 분할 초평면뿐입니다.\n",
    "\n",
    "SVM이 개발되었을 때 간단한 분류 문제에 대해 최고 수준의 성능을 달성했고 광범위한 이론으로 무장된 몇 안 되는 머신 러닝 방법 중 하나가 되었습니다. 또 수학적으로 깊게 분석하기 용이하여 이론을 이해하고 설명하기 쉽습니다. 이런 유용한 특징 때문에 SVM이 오랫동안 머신 러닝 분야에서 매우 큰 인기를 끌었습니다.   \n",
    "\n",
    "하지만 SVM은 대용량의 데이터셋에 확장되기 어렵고 이미지 분류 같은 지각에 관련된 문제에서 좋은 성능을 내지 못했습니다. SVM은 얕은 학습 방법이기 때문에 지각에 관련된 문제에 SVM을 적용하려면 먼저 수동으로 유용한 표현을 추출해야 하는데(이런 단계를 **특성 공학(feature engineering)** 이라고 합니다) 이는 매우 어렵고 불안정합니다. \n",
    "\n",
    "(중략)\n",
    "\n",
    "2014년에 **그래디언트 부스팅 머신(gradient boosting machine)** 이 그 뒤를 이어받았습니다. 랜덤 포레스트와 아주 비슷하게 그래디언트 부스팅 머신은 약한 예측 모델인 결정 트리를 앙상블하는 것을 기반으로 하는 머신 러닝 기법입니다. 이 알고리즘은 이전 모델에서 놓친 데이터 포인트를 보완하는 새로운 모델을 반복적으로 훈련함으로써 머신 러닝 모델을 향상하는 방법인 **그래디언트 부스팅(gradient boosting)** 을 사용합니다. 결정 트리에 그래디언트 부스팅 기법을 적용하면 비슷한 성질을 가지면서도 대부분의 경우에 랜덤 포레스트의 성능을 능가하는 모델을 만듭니다. 이 알고리즘이 오늘날 지각에 관련되지 않은 데이터를 다루기 위한 알고리즘 중 최고는 아니지만 가장 뛰어납니다. 딥러닝을 제외하고 캐글 경연 대회에서 가장 많이 사용되는 기법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝   \n",
    "출처: 책(케라스 창시자에게 배우는 딥러닝)\n",
    "\n",
    "딥러닝은 머신 러닝에서 가장 중요한 단계인 특성 공학을 완전히 자동화하기 때문에 문제를 더 해결하기 쉽게 만들어 줍니다.   \n",
    "얕은 학습인 이전의 머신 러닝 기법은 입력 데이터를 고차원 비선형 투영(SVM)이나 결정 트리 같은 간단한 변환을 통해 하나 또는 2개의 연속된 표현 공간으로만 변환합니다. 하지만 복잡한 문제에 필요한 잘 정제된 표현은 일반적으로 이런 방식으로 얻지 못합니다. 이런 머신 러닝 방법들로 처리하기 용이하게 사람이 초기 입력 데이터를 여러 방식으로 변환해야 합니다. 즉 데이터의 좋은 표현을 수동으로 만들어야 합니다. 이를 **특성 공학(feature engineering)** 이라고 합니다. 그에 반해 딥러닝은 이 단계를 완전히 자동화합니다. 딥러닝을 사용하면 특성을 직접 찾는 대신 한 번에 모든 특성을 학습할 수 있습니다. 머신 러닝 작업 흐름을 매우 단순화시켜 주므로 고도의 다단계 작업 과정을 하나의 간단한 엔드-투-엔드(end-to-end) 딥러닝 모델로 대체할 수 있습니다.   \n",
    "\n",
    "이슈의 핵심이 여러 개의 연속된 표현 층을 가지는 것이라면, 얕은 학습 방법도 딥러닝의 효과를 모사하기 위해 반복적으로 적용할 수 있지 않을까요? 실제로 얕은 학습 방법을 연속적으로 적용하면 각 층의 효과는 빠르게 줄어듭니다. **3개의 층을 가진 모델에서 최적의 첫 번째 표현 층은 하나의 층이나 2개의 층을 가진 모델에서 최적의 첫 번째 층과는 달라야 합니다.** 딥러닝의 변환 능력은 모델이 모든 표현 층을 순차적이 아니라(즉 **탐욕적(greedily)** 방법이 아니라) **동시에 공동으로 학습하게 만듭니다.** 이런 공동 특성 학습 능력 덕택에 모델이 내부 특성 하나에 맞추어질 때마다 이에 의존하는 다른 모든 특성이 사람이 개입하지 않아도 자동으로 변화에 적응하게 됩니다. 모든 학습은 하나의 피드백 신호에 의해 시작됩니다. 즉 모델의 모든 변화는 최종 목표를 따라가게 됩니다. 이 방식은 모델을 많은 중간 영역(층)으로 나누어 복잡하고 추상화된 표현을 학습시킬 수 있기 때문에 얕은 학습 모델을 탐욕적으로 쌓은 것보다 훨씬 강력합니다. 여기에서 각 층은 이전 층에 의존하지 않는 단순한 변환을 수행합니다.\n",
    "\n",
    "딥러닝이 데이터로부터 학습하는 방법에는 두 가지 중요한 특징이 있습니다. **층을 거치면서 점진적으로 더 복잡한 표현이 만들어진다** 는 것과 **이런 점진적인 중간 표현이 공동으로 학습된다** 는 사실입니다. 각 층은 상위 층과 하위 층의 표현이 변함에 따라서 함께 바뀝니다. 이 2개의 특징이 이전의 머신 러닝 접근 방법보다 딥러닝이 훨씬 성공하게 된 이유입니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
