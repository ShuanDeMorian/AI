{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "INFO:tensorflow:Entry Point [tensor2tensor.envs.tic_tac_toe_env:TicTacToeEnv] registered with id [T2TEnv-TicTacToeEnv-v0]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'key transformer_ae already registered in registry models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d0dc4c1f229a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 540\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mTransformerAE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt2t_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT2TModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    541\u001b[0m   \u001b[1;34m\"\"\"Autoencoder-augmented Transformer.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda10.1\\lib\\site-packages\\tensor2tensor\\utils\\registry.py\u001b[0m in \u001b[0;36mregister\u001b[1;34m(self, key_or_value)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[1;31m# Handle if decorator was used without parens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_or_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey_or_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey_or_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda10.1\\lib\\site-packages\\tensor2tensor\\utils\\registry.py\u001b[0m in \u001b[0;36mdecorator\u001b[1;34m(value, key)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda10.1\\lib\\site-packages\\tensor2tensor\\utils\\registry.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m       raise KeyError(\n\u001b[1;32m--> 194\u001b[1;33m           \"key %s already registered in registry %s\" % (key, self._name))\n\u001b[0m\u001b[0;32m    195\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"value must be callable\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'key transformer_ae already registered in registry models'"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2019 The Tensor2Tensor Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"AE Transformer.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import functools\n",
    "import math\n",
    "import os\n",
    "from six.moves import range  # pylint: disable=redefined-builtin\n",
    "\n",
    "from tensor2tensor.layers import common_attention\n",
    "from tensor2tensor.layers import common_image_attention as cia\n",
    "from tensor2tensor.layers import common_layers\n",
    "from tensor2tensor.layers import discretization\n",
    "from tensor2tensor.layers import latent_layers\n",
    "from tensor2tensor.layers import modalities\n",
    "from tensor2tensor.models import transformer\n",
    "from tensor2tensor.utils import beam_search\n",
    "from tensor2tensor.utils import expert_utils\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.utils import t2t_model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "_DO_SUMMARIES = True\n",
    "\n",
    "\n",
    "def residual_conv(x, repeat, k, hparams, name, reuse=None):\n",
    "  \"\"\"A stack of convolution blocks with residual connections.\"\"\"\n",
    "  with tf.variable_scope(name, reuse=reuse):\n",
    "    dilations_and_kernels = [((1, 1), k) for _ in range(3)]\n",
    "    for i in range(repeat):\n",
    "      with tf.variable_scope(\"repeat_%d\" % i):\n",
    "        y = common_layers.conv_block(\n",
    "            common_layers.layer_norm(x, hparams.hidden_size, name=\"lnorm\"),\n",
    "            hparams.hidden_size,\n",
    "            dilations_and_kernels,\n",
    "            padding=\"SAME\",\n",
    "            name=\"residual_conv\")\n",
    "        y = tf.nn.dropout(y, 1.0 - hparams.dropout)\n",
    "        x += y\n",
    "    return x\n",
    "\n",
    "\n",
    "def attend(x, source, hparams, name):\n",
    "  \"\"\"Self-attention layer with source as memory antecedent.\"\"\"\n",
    "  with tf.variable_scope(name):\n",
    "    x = tf.squeeze(x, axis=2)\n",
    "    if len(source.get_shape()) > 3:\n",
    "      source = tf.squeeze(source, axis=2)\n",
    "    source = common_attention.add_timing_signal_1d(source)\n",
    "    y = common_attention.multihead_attention(\n",
    "        common_layers.layer_preprocess(x, hparams), source, None,\n",
    "        hparams.attention_key_channels or hparams.hidden_size,\n",
    "        hparams.attention_value_channels or hparams.hidden_size,\n",
    "        hparams.hidden_size, hparams.num_heads,\n",
    "        hparams.attention_dropout)\n",
    "    res = common_layers.layer_postprocess(x, y, hparams)\n",
    "    return tf.expand_dims(res, axis=2)\n",
    "\n",
    "\n",
    "def decompress_step(source, hparams, first_relu, is_2d, name):\n",
    "  \"\"\"Decompression function.\"\"\"\n",
    "  with tf.variable_scope(name):\n",
    "    shape = common_layers.shape_list(source)\n",
    "    multiplier = 4 if is_2d else 2\n",
    "    kernel = (1, 1) if is_2d else (1, 1)\n",
    "    thicker = common_layers.conv_block(\n",
    "        source, hparams.hidden_size * multiplier, [((1, 1), kernel)],\n",
    "        first_relu=first_relu, name=\"decompress_conv\")\n",
    "    if is_2d:\n",
    "      return tf.depth_to_space(thicker, 2)\n",
    "    return tf.reshape(thicker, [shape[0], shape[1] * 2, 1, hparams.hidden_size])\n",
    "\n",
    "\n",
    "def top_k_softmax(x, k):\n",
    "  \"\"\"Calculate softmax(x), select top-k and rescale to sum to 1.\"\"\"\n",
    "  x = tf.nn.softmax(x)\n",
    "  top_x, _ = tf.nn.top_k(x, k=k+1)\n",
    "  min_top = tf.reduce_min(top_x, axis=-1, keepdims=True)\n",
    "  x = tf.nn.relu((x - min_top) + 1e-12)\n",
    "  x /= tf.reduce_sum(x, axis=-1, keepdims=True)\n",
    "  return x, tf.reduce_max(top_x, axis=-1)\n",
    "\n",
    "\n",
    "def top_k_experts(x, k, hparams):\n",
    "  x_shape = common_layers.shape_list(x)\n",
    "  x_flat = tf.reshape(x, [-1, common_layers.shape_list(x)[-1]])\n",
    "  is_training = hparams.mode == tf.estimator.ModeKeys.TRAIN\n",
    "  gates, load = expert_utils.noisy_top_k_gating(\n",
    "      x_flat, 2 ** hparams.z_size, is_training, k)\n",
    "  gates_shape = [x_shape[0], x_shape[1], x_shape[2], 2 ** hparams.z_size]\n",
    "  gates = tf.reshape(gates, gates_shape)\n",
    "  load_loss = expert_utils.cv_squared(load)\n",
    "  return gates, load_loss\n",
    "\n",
    "\n",
    "def compress(x, c, is_2d, hparams, name):\n",
    "  \"\"\"Compress.\"\"\"\n",
    "  with tf.variable_scope(name):\n",
    "    # Run compression by strided convs.\n",
    "    cur = x\n",
    "    k1 = (3, 3) if is_2d else (3, 1)\n",
    "    k2 = (2, 2) if is_2d else (2, 1)\n",
    "    cur = residual_conv(cur, hparams.num_compress_steps, k1, hparams, \"rc\")\n",
    "    if c is not None and hparams.do_attend_compress:\n",
    "      cur = attend(cur, c, hparams, \"compress_attend\")\n",
    "    for i in range(hparams.num_compress_steps):\n",
    "      if hparams.do_residual_compress:\n",
    "        cur = residual_conv(cur, hparams.num_compress_steps, k1, hparams,\n",
    "                            \"rc_%d\" % i)\n",
    "      cur = common_layers.conv_block(\n",
    "          cur, hparams.hidden_size, [((1, 1), k2)],\n",
    "          strides=k2, name=\"compress_%d\" % i)\n",
    "    return cur\n",
    "\n",
    "\n",
    "def encode(x, x_space, hparams, name):\n",
    "  \"\"\"Transformer preparations and encoder.\"\"\"\n",
    "  with tf.variable_scope(name):\n",
    "    (encoder_input, encoder_self_attention_bias,\n",
    "     ed) = transformer.transformer_prepare_encoder(x, x_space, hparams)\n",
    "    encoder_input = tf.nn.dropout(encoder_input, 1.0 - hparams.dropout)\n",
    "    return transformer.transformer_encoder(\n",
    "        encoder_input, encoder_self_attention_bias, hparams), ed\n",
    "\n",
    "\n",
    "def decode_transformer(encoder_output,\n",
    "                       encoder_decoder_attention_bias,\n",
    "                       targets,\n",
    "                       hparams,\n",
    "                       name,\n",
    "                       task=None,\n",
    "                       causal=True):\n",
    "  \"\"\"Original Transformer decoder.\"\"\"\n",
    "  orig_hparams = hparams\n",
    "  with tf.variable_scope(name):\n",
    "    if task is None:\n",
    "      task = hparams.task\n",
    "    if task == \"translate\":\n",
    "      targets = common_layers.flatten4d3d(targets)\n",
    "\n",
    "      decoder_input, decoder_self_bias = (\n",
    "          transformer.transformer_prepare_decoder(targets, hparams))\n",
    "\n",
    "      decoder_input = tf.nn.dropout(decoder_input,\n",
    "                                    1.0 - hparams.layer_prepostprocess_dropout)\n",
    "\n",
    "      if not causal:\n",
    "        decoder_self_bias *= 0.\n",
    "\n",
    "      decoder_output = transformer.transformer_decoder(\n",
    "          decoder_input,\n",
    "          encoder_output,\n",
    "          decoder_self_bias,\n",
    "          encoder_decoder_attention_bias,\n",
    "          hparams)\n",
    "      decoder_output = tf.expand_dims(decoder_output, axis=2)\n",
    "    else:\n",
    "      assert task == \"image\"\n",
    "      inputs = None\n",
    "      # have to reshape targets as b, 32, 32, 3 * hidden size] beacuse otherwise\n",
    "      # prepare_image will choke\n",
    "      targets = tf.reshape(targets, [tf.shape(targets)[0], hparams.img_len,\n",
    "                                     hparams.img_len,\n",
    "                                     hparams.num_channels*hparams.hidden_size])\n",
    "\n",
    "      # Prepare decoder inputs and bias.\n",
    "      # TODO(nikip): Make prepare_decoder return bias\n",
    "      decoder_input, _, _ = cia.prepare_decoder(targets, hparams)\n",
    "      bias = None\n",
    "\n",
    "      # Add class label to decoder input.\n",
    "      if not hparams.drop_inputs:\n",
    "        decoder_input += tf.reshape(\n",
    "            inputs,\n",
    "            [common_layers.shape_list(targets)[0], 1, 1, hparams.hidden_size])\n",
    "      decoder_output = cia.transformer_decoder_layers(\n",
    "          decoder_input,\n",
    "          encoder_output=None,\n",
    "          num_layers=hparams.num_decoder_layers or hparams.num_hidden_layers,\n",
    "          hparams=hparams,\n",
    "          self_attention_bias=bias,\n",
    "          attention_type=hparams.dec_attention_type,\n",
    "          name=\"decoder\")\n",
    "    decoder_output_shape = common_layers.shape_list(decoder_output)\n",
    "    decoder_output = tf.reshape(decoder_output, [decoder_output_shape[0], -1, 1,\n",
    "                                                 hparams.hidden_size])\n",
    "    # Expand since t2t expects 4d tensors.\n",
    "    hparams = orig_hparams\n",
    "    return decoder_output\n",
    "\n",
    "\n",
    "def multinomial_sample(x, vocab_size, temperature):\n",
    "  \"\"\"Multinomial sampling from a n-dimensional tensor.\"\"\"\n",
    "  if temperature > 0:\n",
    "    samples = tf.multinomial(tf.reshape(x, [-1, vocab_size]) / temperature, 1)\n",
    "  else:\n",
    "    samples = tf.argmax(x, axis=-1)\n",
    "  reshaped_samples = tf.reshape(samples, common_layers.shape_list(x)[:-1])\n",
    "  return tf.to_int32(reshaped_samples)\n",
    "\n",
    "\n",
    "def ae_latent_softmax(latents_pred, latents_discrete, hparams):\n",
    "  \"\"\"Latent prediction and loss.\"\"\"\n",
    "  vocab_size = 2 ** hparams.z_size\n",
    "  if hparams.num_decode_blocks < 2:\n",
    "    latents_logits = tf.layers.dense(latents_pred, vocab_size,\n",
    "                                     name=\"extra_logits\")\n",
    "    if hparams.logit_normalization:\n",
    "      latents_logits *= tf.rsqrt(1e-8 +\n",
    "                                 tf.reduce_mean(tf.square(latents_logits)))\n",
    "\n",
    "    loss = None\n",
    "    if latents_discrete is not None:\n",
    "      if hparams.soft_em:\n",
    "        # latents_discrete is actually one-hot of multinomial samples\n",
    "        assert hparams.num_decode_blocks == 1\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=latents_discrete, logits=latents_logits)\n",
    "      else:\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=latents_discrete, logits=latents_logits)\n",
    "    sample = multinomial_sample(\n",
    "        latents_logits, vocab_size, hparams.sampling_temp)\n",
    "    return sample, loss\n",
    "\n",
    "  # Multi-block case.\n",
    "  vocab_bits = int(math.log(vocab_size, 2))\n",
    "  assert vocab_size == 2**vocab_bits\n",
    "  assert vocab_bits % hparams.num_decode_blocks == 0\n",
    "  block_vocab_size = 2**(vocab_bits // hparams.num_decode_blocks)\n",
    "  latents_logits = [\n",
    "      tf.layers.dense(\n",
    "          latents_pred, block_vocab_size, name=\"extra_logits_%d\" % i)\n",
    "      for i in range(hparams.num_decode_blocks)\n",
    "  ]\n",
    "  loss = None\n",
    "  if latents_discrete is not None:\n",
    "    losses = []\n",
    "    for i in range(hparams.num_decode_blocks):\n",
    "      d = tf.floormod(tf.floordiv(latents_discrete,\n",
    "                                  block_vocab_size**i), block_vocab_size)\n",
    "      losses.append(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          labels=d, logits=latents_logits[i]))\n",
    "    loss = sum(losses)\n",
    "  samples = [multinomial_sample(l, block_vocab_size, hparams.sampling_temp)\n",
    "             for l in latents_logits]\n",
    "  sample = sum([s * block_vocab_size**i for i, s in enumerate(samples)])\n",
    "  return sample, loss\n",
    "\n",
    "\n",
    "def ae_latent_sample_beam(latents_dense_in, inputs, ed, embed, hparams):\n",
    "  \"\"\"Sample from the latent space in the autoencoder.\"\"\"\n",
    "  vocab_size = 2**hparams.z_size\n",
    "  beam_size = 1  # TODO(lukaszkaiser): larger beam sizes seem to work bad.\n",
    "  inputs = tf.tile(inputs, [beam_size, 1, 1])\n",
    "  ed = tf.tile(ed, [beam_size, 1, 1, 1])\n",
    "\n",
    "  def symbols_to_logits_fn(ids):\n",
    "    \"\"\"Go from ids to logits.\"\"\"\n",
    "    ids = tf.expand_dims(ids, axis=2)  # Ids start with added all-zeros.\n",
    "    latents_discrete = tf.pad(ids[:, 1:], [[0, 0], [0, 1], [0, 0]])\n",
    "\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=False):\n",
    "      latents_dense = embed(latents_discrete)\n",
    "      latents_pred = decode_transformer(\n",
    "          inputs, ed, latents_dense, hparams, \"extra\")\n",
    "      logits = tf.layers.dense(latents_pred, vocab_size, name=\"extra_logits\")\n",
    "      current_output_position = common_layers.shape_list(ids)[1] - 1\n",
    "      logits = logits[:, current_output_position, :, :]\n",
    "    return tf.squeeze(logits, axis=[1])\n",
    "\n",
    "  initial_ids = tf.zeros([tf.shape(latents_dense_in)[0]], dtype=tf.int32)\n",
    "  length = tf.shape(latents_dense_in)[1]\n",
    "  ids, _, _ = beam_search.beam_search(\n",
    "      symbols_to_logits_fn, initial_ids, beam_size, length,\n",
    "      vocab_size, alpha=0.0, eos_id=-1, stop_early=False)\n",
    "\n",
    "  res = tf.expand_dims(ids[:, 0, :], axis=2)  # Pick first beam.\n",
    "  return res[:, 1:]  # Remove the added all-zeros from ids.\n",
    "\n",
    "\n",
    "def ae_latent_sample(latents_dense, inputs, ed, embed, iters, hparams):\n",
    "  \"\"\"Sample from the latent space in the autoencoder.\"\"\"\n",
    "  if hparams.num_decode_blocks < 2 and hparams.sampling_temp == 0.0:\n",
    "    # TODO(lukaszkaiser): beam-search only works in non-blocked mode for now.\n",
    "    tf.logging.info(\"Running beam-search for latents with beam size 1.\")\n",
    "    return ae_latent_sample_beam(latents_dense, inputs, ed, embed, hparams)\n",
    "  latents_pred = decode_transformer(inputs, ed, latents_dense, hparams, \"extra\")\n",
    "  latents_discrete, _ = ae_latent_softmax(latents_pred, None, hparams)\n",
    "\n",
    "  def next_bit(latents_discrete, i):\n",
    "    latents_discrete_prev = latents_discrete\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "      latents_dense = embed(latents_discrete)\n",
    "      latents_pred = decode_transformer(\n",
    "          inputs, ed, latents_dense, hparams, \"extra\")\n",
    "      latents_discrete, _ = ae_latent_softmax(latents_pred, None, hparams)\n",
    "      return tf.concat([latents_discrete_prev[:, :(i+1), :],\n",
    "                        latents_discrete[:, (i+1):, :]], axis=1)\n",
    "\n",
    "  for i in range(iters):\n",
    "    latents_discrete = next_bit(latents_discrete, i)\n",
    "  return latents_discrete\n",
    "\n",
    "\n",
    "def ae_transformer_internal(inputs,\n",
    "                            targets,\n",
    "                            target_space,\n",
    "                            hparams,\n",
    "                            cache=None,\n",
    "                            predict_mask=1.0):\n",
    "  \"\"\"AE Transformer, main step used for training.\"\"\"\n",
    "  # Summaries break with the do_refine cond, turn them off in that case.\n",
    "  global _DO_SUMMARIES\n",
    "  if hparams.do_refine:\n",
    "    _DO_SUMMARIES = False\n",
    "\n",
    "  # Prepare.\n",
    "  if inputs is not None:\n",
    "    batch_size = common_layers.shape_list(inputs)[0]\n",
    "  else:\n",
    "    batch_size = common_layers.shape_list(targets)[0]\n",
    "  targets = tf.reshape(targets, [batch_size, -1, 1, hparams.hidden_size])\n",
    "\n",
    "  # Encoder.\n",
    "  if inputs is not None:\n",
    "    inputs = common_layers.flatten4d3d(inputs)\n",
    "    inputs, ed = encode(inputs, target_space, hparams, \"input_enc\")\n",
    "    inputs_ex, ed_ex = inputs, ed\n",
    "  else:\n",
    "    ed, inputs_ex, ed_ex = None, None, None\n",
    "\n",
    "  # Autoencoding.\n",
    "  losses = {\"extra\": tf.constant(0.0), \"latent_pred\": tf.constant(0.0),\n",
    "            \"neg_q_entropy\": tf.constant(0.0)}\n",
    "  if hparams.do_ae:\n",
    "    # flatten here\n",
    "    original_targets = targets\n",
    "    original_targets_shape = tf.shape(original_targets)\n",
    "    if hparams.task == \"image\":\n",
    "      cia.maybe_reshape_4d_to_3d(targets)\n",
    "    if hparams.task == \"translate\":\n",
    "      if inputs is not None:\n",
    "        max_targets_len_from_inputs = tf.concat([inputs, inputs], axis=1)\n",
    "      else:\n",
    "        max_targets_len_from_inputs = targets\n",
    "    else:\n",
    "      assert hparams.task == \"image\"\n",
    "      max_targets_len_from_inputs = targets\n",
    "    if hparams.word_shuffle:\n",
    "      tf.logging.info(\"Using word shuffle with rate = {}\".format(\n",
    "          hparams.word_shuffle))\n",
    "      targets_idx = tf.range(start=0,\n",
    "                             limit=common_layers.shape_list(targets)[1],\n",
    "                             delta=1)\n",
    "      targets_idx = tf.to_float(targets_idx)\n",
    "      noise = tf.random_uniform(shape=common_layers.shape_list(targets_idx),\n",
    "                                minval=0,\n",
    "                                maxval=1 + hparams.word_shuffle)\n",
    "      targets_idx += noise\n",
    "      permutation = tf.contrib.framework.argsort(targets_idx)\n",
    "      targets_permuted = tf.gather(targets, indices=permutation, axis=1)\n",
    "      targets = targets_permuted\n",
    "    targets, _ = common_layers.pad_to_same_length(\n",
    "        targets, max_targets_len_from_inputs,\n",
    "        final_length_divisible_by=2**hparams.num_compress_steps)\n",
    "    # Add positional information\n",
    "    targets_shape = common_layers.shape_list(targets)\n",
    "    targets = tf.reshape(targets, [targets_shape[0], targets_shape[1],\n",
    "                                   targets_shape[3]])\n",
    "    targets = common_attention.add_positional_embedding(\n",
    "        targets, hparams.max_length, name=\"targets_position\")\n",
    "    targets = tf.reshape(targets, shape=targets_shape)\n",
    "    if hparams.word_dropout:\n",
    "      mask = tf.random_uniform(shape=common_layers.shape_list(targets),\n",
    "                               minval=0.0, maxval=1.0)\n",
    "      targets_noisy = tf.where(mask > hparams.word_dropout, targets,\n",
    "                               tf.zeros_like(targets))\n",
    "    else:\n",
    "      targets_noisy = targets\n",
    "\n",
    "    targets_c = compress(targets_noisy, inputs, False, hparams, \"compress\")\n",
    "    if hparams.mode != tf.estimator.ModeKeys.PREDICT:\n",
    "      # Compress and bottleneck.\n",
    "      latents_dense, latents_discrete, extra_loss, embed, neg_q_entropy = (\n",
    "          hparams.bottleneck(inputs=targets_c,\n",
    "                             filter_size=hparams.compress_filter_size,\n",
    "                             mode=hparams.mode,\n",
    "                             name=\"vc\"))\n",
    "      if _DO_SUMMARIES:\n",
    "        tf.summary.histogram(\"b0\", tf.reshape(latents_discrete[:, 0, :], [-1]))\n",
    "      pc = common_layers.inverse_exp_decay(hparams.startup_steps)\n",
    "      pc = pc if hparams.mode == tf.estimator.ModeKeys.TRAIN else 1.0\n",
    "      cond = tf.less(tf.random_uniform([batch_size]), pc)\n",
    "      latents_dense = tf.where(cond, latents_dense, targets_c)\n",
    "      # TODO(lukaszkaiser): return extra losses batchwise, multiply before mean.\n",
    "      losses[\"extra\"] = extra_loss * tf.reduce_mean(tf.to_float(cond))\n",
    "      # Extra loss predicting latent code from input. Discrete only.\n",
    "      if hparams.bottleneck_kind not in [\"dense\", \"vae\"]:\n",
    "        latents_pred = decode_transformer(\n",
    "            inputs_ex, ed_ex,\n",
    "            embed(latents_discrete), hparams, \"extra\",\n",
    "            task=\"translate\")\n",
    "        _, latent_pred_loss = ae_latent_softmax(\n",
    "            latents_pred, tf.stop_gradient(latents_discrete), hparams)\n",
    "\n",
    "        # Scale by latent dimension for summary so we can compare across\n",
    "        # batches.\n",
    "        if _DO_SUMMARIES:\n",
    "          tf.summary.scalar(\"latent_pred_loss_mean\",\n",
    "                            tf.reduce_mean(latent_pred_loss))\n",
    "        if hparams.sum_over_latents:\n",
    "          latent_pred_loss = tf.reduce_sum(latent_pred_loss, [1, 2])\n",
    "\n",
    "        losses[\"latent_pred\"] = tf.reduce_mean(\n",
    "            latent_pred_loss * tf.to_float(cond)) * hparams.prior_scale\n",
    "        losses[\"neg_q_entropy\"] = neg_q_entropy * hparams.entropy_scale\n",
    "      else:\n",
    "        inputs_c = decode_transformer(inputs, ed, targets_c, hparams, \"dec_c\")\n",
    "        losses[\"latent_pred\"] = tf.reduce_mean(\n",
    "            tf.squared_difference(inputs_c, targets_c)) * 20\n",
    "        def bn_inputs():\n",
    "          with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "            bn, _, _, _, _ = hparams.bottleneck(\n",
    "                inputs=inputs_c,\n",
    "                filter_size=hparams.compress_filter_size,\n",
    "                mode=hparams.mode,\n",
    "                name=\"vc\")\n",
    "          return bn\n",
    "        inputs_c = bn_inputs()\n",
    "        ptc = 1.0 - common_layers.inverse_lin_decay(200000) * 0.5\n",
    "        ptc = ptc if hparams.mode == tf.estimator.ModeKeys.TRAIN else 1.0\n",
    "        latents_dense = tf.where(tf.less(tf.random_uniform([batch_size]), ptc),\n",
    "                                 latents_dense, inputs_c)\n",
    "    else:\n",
    "      if hparams.bottleneck_kind in [\"dense\", \"vae\"]:\n",
    "        inputs_c = decode_transformer(inputs, ed, targets_c, hparams, \"dec_c\")\n",
    "        latents_dense, _, _, _, _ = hparams.bottleneck(\n",
    "            inputs=inputs_c,\n",
    "            filter_size=hparams.compress_filter_size,\n",
    "            mode=hparams.mode,\n",
    "            name=\"vc\")\n",
    "      else:\n",
    "        latent_len = common_layers.shape_list(targets_c)[1]\n",
    "        _, _, _, embed, _ = hparams.bottleneck(\n",
    "            inputs=targets_c,\n",
    "            filter_size=hparams.compress_filter_size,\n",
    "            name=\"vc\")\n",
    "        latents_dense = tf.zeros_like(targets_c[:, :latent_len, :, :])\n",
    "        if cache is None:\n",
    "          cache = ae_latent_sample(\n",
    "              latents_dense, inputs_ex, ed_ex, embed, 16, hparams)\n",
    "        latents_dense = embed(cache)\n",
    "    # Postprocess.\n",
    "    d = latents_dense\n",
    "    d_shape = common_layers.shape_list(d)\n",
    "    d = tf.reshape(d, [d_shape[0], d_shape[1], d_shape[3]])\n",
    "    d = common_attention.add_positional_embedding(\n",
    "        d, hparams.max_length, name=\"latents_position\")\n",
    "    d = tf.reshape(d, shape=d_shape)\n",
    "\n",
    "    # decompressing the dense latents\n",
    "    for i in range(hparams.num_compress_steps):\n",
    "      j = hparams.num_compress_steps - i - 1\n",
    "      d = residual_conv(d, 1, (3, 1), hparams, \"decompress_rc_%d\" % j)\n",
    "      if inputs is not None and hparams.do_attend_decompress:\n",
    "        d = attend(d, inputs, hparams, \"decompress_attend_%d\" % j)\n",
    "      d = decompress_step(d, hparams, i > 0, False, \"decompress_%d\" % j)\n",
    "\n",
    "    # Masking.\n",
    "    if hparams.do_mask:\n",
    "      masking = common_layers.inverse_lin_decay(hparams.mask_startup_steps)\n",
    "      masking *= common_layers.inverse_exp_decay(\n",
    "          hparams.mask_startup_steps // 4)  # Not much at start.\n",
    "      if not hparams.do_refine:\n",
    "        masking -= tf.random_uniform([]) * hparams.unmasked_percentage\n",
    "      masking = tf.minimum(tf.maximum(masking, 0.0), 1.0)\n",
    "      if hparams.use_predict_mask:\n",
    "        masking = predict_mask\n",
    "      if hparams.mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        masking = predict_mask\n",
    "      mask = tf.less(masking, tf.random_uniform(\n",
    "          common_layers.shape_list(targets)[:-1]))\n",
    "      mask = tf.expand_dims(tf.to_float(mask), 3)\n",
    "\n",
    "      # targets is always [batch, length, 1, depth]\n",
    "      targets = mask * targets + (1.0 - mask) * d\n",
    "      # reshape back to 4d here\n",
    "      if hparams.task == \"image\":\n",
    "        targets = tf.reshape(targets, original_targets_shape)\n",
    "\n",
    "  res = decode_transformer(inputs, ed, targets, hparams, \"decoder\",\n",
    "                           causal=hparams.causal)\n",
    "  if hparams.do_ae:\n",
    "    if hparams.do_mask and hparams.do_refine:\n",
    "      def refine_res():\n",
    "        # return residual_conv(res, 1, (5, 1), hparams, \"refine\")\n",
    "        r, _ = encode(tf.squeeze(res, axis=[2]),\n",
    "                      target_space, hparams, \"refine_enc\")\n",
    "        return tf.expand_dims(r, axis=2)\n",
    "      masked_batches = tf.reduce_sum(mask, axis=[1, 2, 3])\n",
    "      all_masked = tf.less(masked_batches, 0.1)\n",
    "      res = tf.where(all_masked, refine_res(), res)\n",
    "    # We'll start training the extra model of latents after mask_startup_steps.\n",
    "    nonlatent_steps = hparams.mask_startup_steps\n",
    "    latent_time = tf.less(nonlatent_steps,\n",
    "                          tf.to_int32(tf.train.get_global_step()))\n",
    "    losses[\"latent_pred\"] *= tf.to_float(latent_time)\n",
    "\n",
    "  # res was generated from padded targets, which means it has some extra\n",
    "  # elements. These can cause shape problems when computing loss with respect to\n",
    "  # the original (unpadded) targets. So we remove their extra elements here.\n",
    "  res = res[:, :original_targets_shape[1], :, :]\n",
    "\n",
    "  data_dim = common_layers.shape_list(res)[1]\n",
    "  latent_dim = common_layers.shape_list(targets_c)[1]\n",
    "  return res, losses, cache, data_dim, latent_dim\n",
    "\n",
    "\n",
    "@registry.register_model\n",
    "class TransformerAE(t2t_model.T2TModel):\n",
    "  \"\"\"Autoencoder-augmented Transformer.\"\"\"\n",
    "\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super(TransformerAE, self).__init__(*args, **kwargs)\n",
    "    self.predict_mask = 1.0\n",
    "\n",
    "    # Define bottleneck function\n",
    "    self._hparams.bottleneck = functools.partial(\n",
    "        discretization.discrete_bottleneck,\n",
    "        hidden_size=self._hparams.hidden_size,\n",
    "        z_size=self._hparams.z_size,\n",
    "        filter_size=self._hparams.filter_size,\n",
    "        bottleneck_kind=self._hparams.bottleneck_kind,\n",
    "        num_blocks=self._hparams.num_blocks,\n",
    "        num_residuals=self.hparams.num_residuals,\n",
    "        reshape_method=self._hparams.reshape_method,\n",
    "        beta=self._hparams.beta,\n",
    "        ema=self._hparams.ema,\n",
    "        epsilon=self._hparams.epsilon,\n",
    "        decay=self._hparams.decay,\n",
    "        random_top_k=self._hparams.random_top_k,\n",
    "        soft_em=self.hparams.soft_em,\n",
    "        num_samples=self.hparams.num_samples,\n",
    "        softmax_k=self._hparams.softmax_k,\n",
    "        temperature_warmup_steps=self._hparams.temperature_warmup_steps,\n",
    "        do_hard_gumbel_softmax=self._hparams.do_hard_gumbel_softmax,\n",
    "        num_flows=self._hparams.num_flows,\n",
    "        approximate_gs_entropy=self._hparams.approximate_gs_entropy,\n",
    "        discrete_mix=self._hparams.d_mix,\n",
    "        noise_dev=self._hparams.noise_dev,\n",
    "        startup_steps=self.hparams.startup_steps,\n",
    "        summary=_DO_SUMMARIES)\n",
    "    # Set the discretization bottleneck specific things here\n",
    "    if self._hparams.bottleneck_kind in [\"dvq\", \"gumbel-softmax-dvq\"]:\n",
    "      z_size_per_residual = self._hparams.z_size / self._hparams.num_residuals\n",
    "      block_dim = int(self._hparams.hidden_size // self._hparams.num_blocks)\n",
    "      block_v_size = 2**(z_size_per_residual / self._hparams.num_blocks)\n",
    "      block_v_size = int(block_v_size)\n",
    "\n",
    "      if self._hparams.reshape_method == \"project\":\n",
    "        tf.logging.info(\"Using projections for DVQ\")\n",
    "        tf.logging.info(\"Trainable projections = {}\".format(\n",
    "            self._hparams.trainable_projections))\n",
    "\n",
    "        projection_tensors = tf.get_variable(\n",
    "            name=\"projection\",\n",
    "            shape=[\n",
    "                self._hparams.num_residuals, self._hparams.num_blocks,\n",
    "                self._hparams.hidden_size, block_dim\n",
    "            ],\n",
    "            initializer=tf.initializers.glorot_uniform(),\n",
    "            trainable=self._hparams.trainable_projections)\n",
    "\n",
    "        self._hparams.bottleneck = functools.partial(\n",
    "            self._hparams.bottleneck, projection_tensors=projection_tensors)\n",
    "      elif self._hparams.reshape_method == \"slice\":\n",
    "        tf.logging.info(\"Using slices for DVQ\")\n",
    "      else:\n",
    "        raise ValueError(\"Unknown reshape method\")\n",
    "\n",
    "      means = tf.get_variable(\n",
    "          name=\"means\",\n",
    "          shape=[\n",
    "              self._hparams.num_residuals, self._hparams.num_blocks,\n",
    "              block_v_size, block_dim\n",
    "          ],\n",
    "          initializer=tf.uniform_unit_scaling_initializer())\n",
    "\n",
    "      # Create the shadow variables if we are using EMA\n",
    "      ema_count = None\n",
    "      ema_means = None\n",
    "      if self._hparams.ema:\n",
    "        ema_count = []\n",
    "        for i in range(self._hparams.num_residuals):\n",
    "          ema_count_i = tf.get_variable(\n",
    "              \"ema_count_{}\".format(i),\n",
    "              [self._hparams.num_blocks, block_v_size],\n",
    "              initializer=tf.constant_initializer(0),\n",
    "              trainable=False)\n",
    "          ema_count.append(ema_count_i)\n",
    "        with tf.colocate_with(means):\n",
    "          ema_means = []\n",
    "          for i in range(self._hparams.num_residuals):\n",
    "            ema_means_i = tf.get_variable(\n",
    "                \"ema_means_{}\".format(i),\n",
    "                [self._hparams.num_blocks, block_v_size, block_dim],\n",
    "                initializer=(lambda shape, dtype=None, partition_info=None,  # pylint: disable=g-long-lambda\n",
    "                                    verify_shape=None:\n",
    "                             means.initialized_value()[i]),\n",
    "                trainable=False)\n",
    "            ema_means.append(ema_means_i)\n",
    "\n",
    "      # Update bottleneck\n",
    "      self._hparams.bottleneck = functools.partial(\n",
    "          self._hparams.bottleneck,\n",
    "          means=means,\n",
    "          ema_count=ema_count,\n",
    "          ema_means=ema_means)\n",
    "\n",
    "  def body(self, features):\n",
    "    inputs = features[\"inputs\"] if \"inputs\" in features else None\n",
    "    if self._hparams.drop_inputs:\n",
    "      inputs = None\n",
    "    reuse = \"cache_raw\" in features\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=reuse):\n",
    "      res, loss, _, self._data_dim, self._latent_dim = ae_transformer_internal(\n",
    "          inputs,\n",
    "          features[\"targets\"],\n",
    "          features[\"target_space_id\"],\n",
    "          self._hparams,\n",
    "          features.get(\"cache_raw\", None),\n",
    "          predict_mask=self.predict_mask)\n",
    "      return res, loss\n",
    "\n",
    "  def prepare_features_for_infer(self, features):\n",
    "    if self._hparams.do_mask or not self._hparams.do_ae:\n",
    "      return features\n",
    "    beam_batch_size = self._decode_hparams.beam_size\n",
    "    beam_batch_size *= self._decode_hparams.batch_size\n",
    "    inputs = tf.zeros([beam_batch_size, 1, 1, self._hparams.hidden_size])\n",
    "    inputs = inputs if \"inputs\" in features else None\n",
    "    if self._hparams.drop_inputs or not self.has_input:\n",
    "      inputs = None\n",
    "    targets = tf.zeros([beam_batch_size, 1, 1, self._hparams.hidden_size])\n",
    "    with tf.variable_scope(\"body\"):\n",
    "      _, _, cache, _, _ = ae_transformer_internal(\n",
    "          inputs, targets, features[\"target_space_id\"], self._hparams)\n",
    "    features[\"cache_raw\"] = cache\n",
    "\n",
    "  def infer(self, features=None, decode_length=50, beam_size=1, top_beams=1,\n",
    "            alpha=0.0, use_tpu=False):\n",
    "    \"\"\"Produce predictions from the model.\"\"\"\n",
    "    if not self._hparams.do_mask:\n",
    "      infer_out = super(TransformerAE, self).infer(\n",
    "          features, decode_length, beam_size, top_beams, alpha, use_tpu=use_tpu)\n",
    "      return infer_out[\"outputs\"]\n",
    "    if not features:\n",
    "      features = {}\n",
    "    inputs_old = None\n",
    "    if \"inputs\" in features and len(features[\"inputs\"].shape) < 4:\n",
    "      inputs_old = features[\"inputs\"]\n",
    "      features[\"inputs\"] = tf.expand_dims(features[\"inputs\"], 2)\n",
    "\n",
    "    # Create an initial targets tensor.\n",
    "    if \"partial_targets\" in features:\n",
    "      initial_output = tf.convert_to_tensor(features[\"partial_targets\"])\n",
    "    else:\n",
    "      # inputs might not be present in features (e.g.: language modeling),\n",
    "      # in which case we fallback to 'infer_targets' for calculating initial\n",
    "      # input shape, type, etc.\n",
    "      inputs_or_targets = features.get(\"inputs\", features.get(\"infer_targets\"))\n",
    "      batch_size = common_layers.shape_list(inputs_or_targets)[0]\n",
    "      length = common_layers.shape_list(inputs_or_targets)[1]\n",
    "      hidden_dim = common_layers.shape_list(inputs_or_targets)[-1]\n",
    "      target_length = tf.to_int32(2.0 * tf.to_float(length))\n",
    "      initial_output = tf.zeros((batch_size, target_length, 1, hidden_dim),\n",
    "                                dtype=inputs_or_targets.dtype)\n",
    "\n",
    "    features[\"targets\"] = initial_output\n",
    "    logits, _ = self(features)  # pylint: disable=not-callable\n",
    "    # this should only happen if we're doing target_modality not real\n",
    "    if inputs_or_targets.dtype == tf.float32:\n",
    "      samples = logits\n",
    "    else:\n",
    "      samples = tf.argmax(logits, axis=-1)\n",
    "\n",
    "    # More steps.\n",
    "    self.predict_mask = 0.0  # Use the provided targets this time.\n",
    "    how_many_more_steps = 0  # Set to 1 or more for Gibbs-like sampling.\n",
    "    for _ in range(how_many_more_steps):\n",
    "      with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "        features[\"targets\"] = samples\n",
    "        logits, _ = self(features)  # pylint: disable=not-callable\n",
    "        if inputs_or_targets.dtype == tf.float32:\n",
    "          # When target_modality is real, the last axis does not represent\n",
    "          # classes, so it should not be argmax'ed\n",
    "          samples = logits\n",
    "        else:\n",
    "          samples = tf.argmax(logits, axis=-1)\n",
    "\n",
    "    self.predict_mask = 1.0\n",
    "    if inputs_old is not None:  # Restore to not confuse Estimator.\n",
    "      features[\"inputs\"] = inputs_old\n",
    "    return samples\n",
    "\n",
    "  def estimator_spec_eval(self, features, logits, labels, loss, losses_dict):\n",
    "    \"\"\"Constructs `tf.estimator.EstimatorSpec` for EVAL (evaluation) mode.\"\"\"\n",
    "    estimator_spec = super(TransformerAE, self).estimator_spec_eval(\n",
    "        features, logits, labels, loss, losses_dict)\n",
    "    if common_layers.is_xla_compiled():\n",
    "      # For TPUs (and XLA more broadly?), do not add summary hooks that depend\n",
    "      # on losses; they are not supported.\n",
    "      return estimator_spec\n",
    "\n",
    "    summary_op = tf.get_collection(tf.GraphKeys.SUMMARIES, scope=\"losses\")\n",
    "    summary_op.extend(tf.get_collection(tf.GraphKeys.SUMMARIES, scope=\"loss\"))\n",
    "    summary_op.append(tf.summary.scalar(\"loss\", loss))\n",
    "    summary_saver_hook = tf.train.SummarySaverHook(\n",
    "        save_steps=100,\n",
    "        summary_op=summary_op,\n",
    "        output_dir=os.path.join(self.hparams.model_dir, \"eval\"))\n",
    "\n",
    "    hooks = list(estimator_spec.evaluation_hooks)\n",
    "    hooks.append(summary_saver_hook)\n",
    "    return estimator_spec._replace(evaluation_hooks=hooks)\n",
    "\n",
    "  def _summarize_losses(self, losses_dict):\n",
    "    \"\"\"Adds `tf.summary`s to all terms in the losses dictionary.\"\"\"\n",
    "    super(TransformerAE, self)._summarize_losses(losses_dict)\n",
    "    nats_per_dim, bits_per_dim = latent_layers.compute_nats_and_bits_per_dim(\n",
    "        data_dim=self._data_dim,\n",
    "        latent_dim=self._latent_dim,\n",
    "        average_reconstruction=losses_dict[\"training\"],\n",
    "        average_prior=losses_dict[\"latent_pred\"])\n",
    "    tf.summary.scalar(\"loss/nats_per_dim\", nats_per_dim)\n",
    "    tf.summary.scalar(\"loss/bits_per_dim\", bits_per_dim)\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ae_small():\n",
    "  \"\"\"Set of hyperparameters.\"\"\"\n",
    "  hparams = transformer.transformer_small()\n",
    "  hparams.batch_size = 2048\n",
    "  hparams.learning_rate = 0.2\n",
    "  hparams.learning_rate_warmup_steps = 4000\n",
    "  hparams.num_hidden_layers = 3\n",
    "  hparams.hidden_size = 384\n",
    "  hparams.filter_size = 2048\n",
    "  hparams.add_hparam(\"compress_filter_size\", 2048 * 2)\n",
    "  hparams.label_smoothing = 0.0\n",
    "  hparams.optimizer = \"adam\"  # Can be unstable, maybe try Adam.\n",
    "  hparams.optimizer_adam_epsilon = 1e-9\n",
    "  hparams.optimizer_adam_beta1 = 0.9\n",
    "  hparams.optimizer_adam_beta2 = 0.997  # Needs tuning, try 0.98 to 0.999.\n",
    "  hparams.add_hparam(\"z_size\", 14)\n",
    "  hparams.add_hparam(\"noise_dev\", 0.5)\n",
    "  hparams.add_hparam(\"d_mix\", 0.5)\n",
    "  hparams.add_hparam(\"logit_normalization\", True)\n",
    "  hparams.add_hparam(\"word_dropout\", 0.)\n",
    "  # Bottleneck kinds supported: dense, vae, semhash, gumbel-softmax, dvq.\n",
    "  hparams.add_hparam(\"bottleneck_kind\", \"semhash\")\n",
    "  hparams.add_hparam(\"num_blocks\", 1)\n",
    "  hparams.add_hparam(\"num_decode_blocks\", 1)\n",
    "  # Add an hparam for number of reiduals\n",
    "  hparams.add_hparam(\"num_residuals\", 1)\n",
    "  # Reshape method for DVQ: slice, project\n",
    "  hparams.add_hparam(\"word_shuffle\", 0.5)\n",
    "  hparams.add_hparam(\"causal\", True)\n",
    "  hparams.add_hparam(\"reshape_method\", \"slice\")\n",
    "  hparams.add_hparam(\"trainable_projections\", False)\n",
    "  hparams.add_hparam(\"unmasked_percentage\", 0.1)\n",
    "  hparams.add_hparam(\"do_ae\", True)\n",
    "  hparams.add_hparam(\"do_mask\", True)\n",
    "  hparams.add_hparam(\"use_predict_mask\", True)\n",
    "  hparams.add_hparam(\"do_refine\", False)\n",
    "  hparams.add_hparam(\"do_attend_compress\", False)\n",
    "  hparams.add_hparam(\"do_attend_decompress\", True)\n",
    "  hparams.add_hparam(\"do_residual_compress\", False)\n",
    "  hparams.add_hparam(\"drop_inputs\", False)\n",
    "  hparams.add_hparam(\"v_size\", 1024*64)\n",
    "  hparams.add_hparam(\"max_context_length\", 64)\n",
    "  hparams.add_hparam(\"num_compress_steps\", 3)\n",
    "  hparams.add_hparam(\"startup_steps\", 10000)\n",
    "  hparams.add_hparam(\"mask_startup_steps\", 50000)\n",
    "  hparams.add_hparam(\"z_dropout\", 0.1)\n",
    "  hparams.add_hparam(\"is_2d\", 0)\n",
    "  hparams.add_hparam(\"softmax_k\", 0)\n",
    "  hparams.add_hparam(\"decode_autoregressive\", True)\n",
    "  hparams.add_hparam(\"do_vae\", True)\n",
    "  hparams.add_hparam(\"bit_vae\", True)\n",
    "  hparams.add_hparam(\"beta\", 0.25)\n",
    "  hparams.add_hparam(\"epsilon\", 1e-5)\n",
    "  hparams.add_hparam(\"decay\", 0.999)\n",
    "  hparams.add_hparam(\"ema\", True)\n",
    "  hparams.add_hparam(\"random_top_k\", 1)\n",
    "  hparams.add_hparam(\"soft_em\", False)\n",
    "  hparams.add_hparam(\"num_samples\", 10)\n",
    "  hparams.add_hparam(\"inv_temp\", 1.0)\n",
    "  hparams.add_hparam(\"entropy_scale\", 0.0)\n",
    "  hparams.add_hparam(\"prior_scale\", 1.0)\n",
    "  hparams.add_hparam(\"do_hard_gumbel_softmax\", False)\n",
    "  hparams.add_hparam(\"num_flows\", 0)\n",
    "  hparams.add_hparam(\"approximate_gs_entropy\", False)\n",
    "  hparams.add_hparam(\"temperature_warmup_steps\", 150000)\n",
    "  hparams.add_hparam(\"sum_over_latents\", False)\n",
    "  hparams.force_full_predict = True\n",
    "\n",
    "  # task params\n",
    "  hparams.add_hparam(\"task\", \"translate\")  # translate or image tasks supported\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def imagetransformer_ae_cifar():\n",
    "  \"\"\"Hyperparameters for CIFAR-10 experiments.\"\"\"\n",
    "  hparams = transformer_ae_small()\n",
    "  hparams.filter_size = 512\n",
    "  hparams.num_compress_steps = 3\n",
    "  hparams.startup_steps = 10000\n",
    "  hparams.is_2d = 0\n",
    "  hparams.learning_rate_warmup_steps = 8000\n",
    "  hparams.learning_rate = 0.2\n",
    "  hparams.hidden_size = 512\n",
    "  hparams.batch_size = 1\n",
    "  hparams.max_length = 256\n",
    "  hparams.dropout = 0.0\n",
    "  hparams.clip_grad_norm = 0.  # i.e. no gradient clipping\n",
    "  hparams.optimizer_adam_epsilon = 1e-9\n",
    "  hparams.learning_rate_decay_scheme = \"noam\"\n",
    "  hparams.learning_rate = 0.1\n",
    "  hparams.initializer_gain = 0.2\n",
    "  hparams.num_hidden_layers = 6\n",
    "  hparams.initializer = \"uniform_unit_scaling\"\n",
    "  hparams.weight_decay = 0.0\n",
    "  hparams.optimizer_adam_beta1 = 0.9\n",
    "  hparams.optimizer_adam_beta2 = 0.98\n",
    "  hparams.label_smoothing = 0.0\n",
    "  hparams.norm_type = \"layer\"\n",
    "  hparams.layer_prepostprocess_dropout = 0.0\n",
    "  hparams.num_heads = 8\n",
    "  hparams.task = \"image\"\n",
    "  hparams.ffn_layer = \"conv_hidden_relu\"\n",
    "  # All hyperparameters ending in \"dropout\" are automatically set to 0.0\n",
    "  # when not in training mode.\n",
    "  hparams.attention_dropout = 0.0\n",
    "  hparams.relu_dropout = 0.\n",
    "  hparams.pos = \"timing\"  # timing, none\n",
    "  hparams.nbr_decoder_problems = 1\n",
    "  hparams.num_output_layers = 3\n",
    "  # TODO(trandustin): semhash doesn't work if filter_size != hidden_size. For\n",
    "  # now, set default to dvq.\n",
    "  hparams.bottleneck_kind = \"dvq\"\n",
    "  hparams.add_hparam(\"block_size\", 1)\n",
    "\n",
    "  # dilated attention based flags\n",
    "  hparams.add_hparam(\"gap_sizes\", [2, 4, 8, 16, 32, 64, 2, 4, 8, 16, 32, 64])\n",
    "  hparams.add_hparam(\"dilated_attention\", False)\n",
    "\n",
    "  # image size related flags\n",
    "  # assuming that the image has same height and width\n",
    "  hparams.add_hparam(\"img_len\", 32)\n",
    "  hparams.add_hparam(\"num_channels\", 3)\n",
    "  # Local attention params\n",
    "  hparams.add_hparam(\"local_and_global_att\", False)\n",
    "  hparams.add_hparam(\"block_length\", 256)\n",
    "  hparams.add_hparam(\"block_width\", 128)\n",
    "  hparams.num_encoder_layers = 4\n",
    "  hparams.num_decoder_layers = 12\n",
    "  hparams.add_hparam(\"dec_attention_type\", cia.AttentionType.LOCAL_1D)\n",
    "  hparams.add_hparam(\"block_raster_scan\", False)\n",
    "  hparams.add_hparam(\"shared_rel\", False)\n",
    "\n",
    "  # multipos attention params\n",
    "  hparams.add_hparam(\"q_filter_width\", 1)\n",
    "  hparams.add_hparam(\"kv_filter_width\", 1)\n",
    "\n",
    "  hparams.add_hparam(\"unconditional\", False)  # unconditional generation\n",
    "\n",
    "  hparams.bottom[\"targets\"] = modalities.image_channel_embeddings_bottom\n",
    "  hparams.top[\"targets\"] = modalities.image_channel_embeddings_top\n",
    "  hparams.drop_inputs = True\n",
    "  hparams.do_attend_compress = False\n",
    "  hparams.do_attend_decompress = False\n",
    "  return hparams\n",
    "\n",
    "\n",
    "def imagetransformer_ae_imagenet():\n",
    "  \"\"\"For 64x64 ImageNet. ~56M trainable variables.\"\"\"\n",
    "  hparams = imagetransformer_ae_cifar()\n",
    "  hparams.max_length = int(64 * 64 * 3)\n",
    "  hparams.img_len = 64\n",
    "  hparams.num_heads = 4  # Heads are expensive on TPUs.\n",
    "  # Reduce architecture from 32x32 CIFAR-10 in order to fit in memory.\n",
    "  hparams.num_decoder_layers = 8\n",
    "  hparams.num_compress_steps = 2\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ae_base():\n",
    "  \"\"\"Set of hyperparameters.\"\"\"\n",
    "  hparams = transformer_ae_small()\n",
    "  hparams.batch_size = 2048\n",
    "  hparams.hidden_size = 512\n",
    "  hparams.filter_size = 4096\n",
    "  hparams.num_hidden_layers = 6\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ae_a3():\n",
    "  \"\"\"Set of hyperparameters.\"\"\"\n",
    "  hparams = transformer_ae_base()\n",
    "  hparams.batch_size = 4096\n",
    "  hparams.layer_prepostprocess_dropout = 0.3\n",
    "  hparams.optimizer = \"Adafactor\"\n",
    "  hparams.learning_rate = 0.25\n",
    "  hparams.learning_rate_warmup_steps = 10000\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ae_a6():\n",
    "  \"\"\"Best hparams for transformer with semhash.\"\"\"\n",
    "  hparams = transformer_ae_a3()\n",
    "  hparams.optimizer = \"adam\"\n",
    "  hparams.noise_dev = 0.5\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ae_a8():\n",
    "  \"\"\"Set of hyperparameters.\"\"\"\n",
    "  hparams = transformer_ae_a3()\n",
    "  hparams.optimizer = \"Adafactor\"\n",
    "  hparams.noise_dev = 0.5\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ae_base_tpu():\n",
    "  \"\"\"Base config adjusted for TPU.\"\"\"\n",
    "  hparams = transformer_ae_base()\n",
    "  transformer.update_hparams_for_tpu(hparams)\n",
    "  hparams.batch_size = 512\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ae_base_noatt():\n",
    "  \"\"\"Set of hyperparameters.\"\"\"\n",
    "  hparams = transformer_ae_base()\n",
    "  hparams.reshape_method = \"slice\"\n",
    "  hparams.bottleneck_kind = \"dvq\"\n",
    "  hparams.hidden_size = 512\n",
    "  hparams.num_blocks = 1\n",
    "  hparams.num_decode_blocks = 1\n",
    "  hparams.z_size = 12\n",
    "  hparams.do_attend_decompress = False\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ae_small_noatt():\n",
    "  \"\"\"Set of hyperparameters.\"\"\"\n",
    "  hparams = transformer_ae_small()\n",
    "  hparams.reshape_method = \"slice\"\n",
    "  hparams.bottleneck_kind = \"dvq\"\n",
    "  hparams.hidden_size = 512\n",
    "  hparams.num_blocks = 1\n",
    "  hparams.num_decode_blocks = 1\n",
    "  hparams.z_size = 12\n",
    "  hparams.do_attend_decompress = False\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ae_base_ablation_1():\n",
    "  hparams = transformer_ae_base_noatt()\n",
    "  hparams.soft_em = True\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ae_base_ablation_2():\n",
    "  hparams = transformer_ae_base_ablation_1()\n",
    "  hparams.entropy_scale = 0.1\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ae_base_ablation_3():\n",
    "  hparams = transformer_ae_base_ablation_2()\n",
    "  hparams.prior_scale = 0.1\n",
    "  hparams.entropy_scale = 0.1\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ae_base_ablation_4():\n",
    "  hparams = transformer_ae_base_ablation_3()\n",
    "  hparams.entropy_scale = 0.0\n",
    "  hparams.prior_scale = 1.0\n",
    "  hparams.bottleneck_kind = \"gumbel-softmax-dvq\"\n",
    "  hparams.do_hard_gumbel_softmax = True\n",
    "  hparams.approximate_gs_entropy = True\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ae_base_ablation_5():\n",
    "  hparams = transformer_ae_base_ablation_4()\n",
    "  hparams.do_hard_gumbel_softmax = False\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_ae_base_iaf():\n",
    "  hparams = transformer_ae_base_ablation_5()\n",
    "  hparams.num_flows = 1\n",
    "  hparams.num_samples = 1\n",
    "  return hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    --------------------------------------------------------------------------------------------------------\n",
    "@registry.register_hparams\n",
    "def transformer_ae_my():\n",
    "  \"\"\"Set of hyperparameters.\"\"\"\n",
    "  hparams = transformer_ae_small()\n",
    "  hparams.batch_size = 256\n",
    "  hparams.hidden_size = 128\n",
    "  hparams.filter_size = 512\n",
    "  hparams.num_hidden_layers = 6\n",
    "  return hparams\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensor2tensor.bin import t2t_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to operator (<ipython-input-8-b58a18ef9795>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-b58a18ef9795>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    t2t-trainer \\\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m can't assign to operator\n"
     ]
    }
   ],
   "source": [
    "t2t-trainer \\\n",
    "  --generate_data \\\n",
    "  --data_dir=t2t/data \\\n",
    "  --output_dir=t2t/train/LM_ptb_characters \\\n",
    "  --problem=languagemodel_ptb_characters \\\n",
    "  --model=transformer_ae \\\n",
    "  --hparams_set=transformer_ae_my \\\n",
    "  --train_steps=1000 \\\n",
    "  --eval_steps=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to operator (<ipython-input-10-1b5cfa24d268>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-1b5cfa24d268>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    t2t_trainer --generate_data --data_dir=t2t/data --output_dir=t2t/train/LM_ptb_characters --problem=languagemodel_ptb_characters --model=transformer_ae --hparams_set=transformer_ae_my --train_steps=1000 --eval_steps=100\u001b[0m\n\u001b[1;37m                                                                                                                                                                                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m can't assign to operator\n"
     ]
    }
   ],
   "source": [
    "t2t_trainer --generate_data --data_dir=t2t/data --output_dir=t2t/train/LM_ptb_characters --problem=languagemodel_ptb_characters --model=transformer_ae --hparams_set=transformer_ae_my --train_steps=1000 --eval_steps=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2t_trainer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
